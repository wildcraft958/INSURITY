{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6abc64",
   "metadata": {},
   "source": [
    "# Comprehensive Telematics Driving Behavior Analysis\n",
    "## Feature Engineering, Classification, and Behavior Scoring for Insurance Applications\n",
    "\n",
    "This notebook provides a complete pipeline for analyzing driving behavior from telematics sensor data, combining:\n",
    "- Advanced feature engineering from accelerometer and gyroscope signals\n",
    "- Multiple machine learning and deep learning classification models\n",
    "- Behavior score calculation for insurance risk assessment\n",
    "- Model comparison and evaluation metrics\n",
    "\n",
    "**Key Features:**\n",
    "- Time and frequency domain feature extraction\n",
    "- Signal smoothing and noise reduction\n",
    "- 3-class driving behavior classification (SLOW, NORMAL, AGGRESSIVE)\n",
    "- Comprehensive behavior scoring algorithm\n",
    "- Export functionality for production systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d61bb6",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Signal processing\n",
    "from scipy import signal\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import (classification_report, accuracy_score, ConfusionMatrixDisplay, \n",
    "                           confusion_matrix, precision_score, recall_score, roc_curve, \n",
    "                           roc_auc_score, balanced_accuracy_score, silhouette_score)\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Scikit-learn version: {__import__('sklearn').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f490e",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data_path = 'train_motion_data.csv'\n",
    "test_data_path = 'test_motion_data.csv'\n",
    "\n",
    "# Try different possible paths\n",
    "possible_paths = [\n",
    "    train_data_path,\n",
    "    f'data/{train_data_path}',\n",
    "    f'/home/bakasur/INSURITY/{train_data_path}',\n",
    "    f'/kaggle/input/driving-behavior/{train_data_path}'\n",
    "]\n",
    "\n",
    "train_data = None\n",
    "for path in possible_paths:\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            train_data = pd.read_csv(path)\n",
    "            print(f\"Successfully loaded training data from: {path}\")\n",
    "            break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "if train_data is None:\n",
    "    print(\"Could not find training data. Please ensure the file is in the correct location.\")\n",
    "    # Create sample data for demonstration\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 10000\n",
    "    train_data = pd.DataFrame({\n",
    "        'Timestamp': range(n_samples),\n",
    "        'AccX': np.random.normal(0, 1, n_samples),\n",
    "        'AccY': np.random.normal(0, 1, n_samples),\n",
    "        'AccZ': np.random.normal(9.8, 1, n_samples),\n",
    "        'GyroX': np.random.normal(0, 0.5, n_samples),\n",
    "        'GyroY': np.random.normal(0, 0.5, n_samples),\n",
    "        'GyroZ': np.random.normal(0, 0.5, n_samples),\n",
    "        'Class': np.random.choice(['SLOW', 'NORMAL', 'AGGRESSIVE'], n_samples, p=[0.3, 0.4, 0.3])\n",
    "    })\n",
    "\n",
    "print(f\"Dataset shape: {train_data.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab50119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration and preprocessing\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(f\"Shape: {train_data.shape}\")\n",
    "print(f\"Columns: {list(train_data.columns)}\")\n",
    "print(f\"Data types:\\n{train_data.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{train_data.isnull().sum()}\")\n",
    "print(f\"\\nClass distribution:\\n{train_data['Class'].value_counts()}\")\n",
    "print(f\"\\nClass distribution (%):\\n{train_data['Class'].value_counts(normalize=True) * 100}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n=== Statistical Summary ===\")\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efa365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "df = train_data.copy()\n",
    "\n",
    "# Ensure data is sorted by timestamp\n",
    "df = df.sort_values('Timestamp').reset_index(drop=True)\n",
    "\n",
    "# Convert timestamp to proper format\n",
    "df['Timestamp'] = (df.index + 1) / 2  # Assuming 2 samples per second\n",
    "\n",
    "# Define sensor columns\n",
    "sensor_columns = ['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ']\n",
    "\n",
    "# Normalize sensor data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[sensor_columns] = scaler.fit_transform(df[sensor_columns])\n",
    "\n",
    "# Map classes to numeric values\n",
    "class_mapping = {'SLOW': 0, 'NORMAL': 1, 'AGGRESSIVE': 2}\n",
    "df['Class_numeric'] = df['Class'].map(class_mapping)\n",
    "\n",
    "# Add derived features\n",
    "df['Acc_magnitude'] = np.sqrt(df['AccX']**2 + df['AccY']**2 + df['AccZ']**2)\n",
    "df['Gyro_magnitude'] = np.sqrt(df['GyroX']**2 + df['GyroY']**2 + df['GyroZ']**2)\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n",
    "print(f\"Processed dataset shape: {df.shape}\")\n",
    "print(f\"Sensor columns normalized: {sensor_columns}\")\n",
    "print(f\"Added magnitude features: ['Acc_magnitude', 'Gyro_magnitude']\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['Class'].value_counts().plot(kind='bar', color=['green', 'blue', 'red'])\n",
    "plt.title('Class Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['Class'].value_counts(normalize=True).plot(kind='pie', autopct='%1.1f%%', colors=['green', 'blue', 'red'])\n",
    "plt.title('Class Distribution (%)')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec1f025",
   "metadata": {},
   "source": [
    "# 3. Signal Smoothing and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae263b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define smoothing functions\n",
    "def rolling_average_smoothing(data, window=5):\n",
    "    \"\"\"Apply rolling average smoothing\"\"\"\n",
    "    return data.rolling(window=window, center=True).mean()\n",
    "\n",
    "def exponential_smoothing(data, alpha=0.1):\n",
    "    \"\"\"Apply exponential smoothing\"\"\"\n",
    "    return data.ewm(alpha=alpha).mean()\n",
    "\n",
    "def gaussian_smoothing(data, window=5, std=1):\n",
    "    \"\"\"Apply Gaussian smoothing\"\"\"\n",
    "    return data.rolling(window=window, win_type='gaussian', center=True).mean(std=std)\n",
    "\n",
    "# Apply different smoothing techniques\n",
    "sampling_rate = 2  # 2 samples per second\n",
    "window_size = int(4 * sampling_rate)  # 4-second windows\n",
    "overlap = int(window_size * 0.25)  # 25% overlap\n",
    "\n",
    "# Create smoothed versions of the data\n",
    "df_smoothed = df.copy()\n",
    "\n",
    "# Apply rolling average to sensor columns\n",
    "for col in sensor_columns:\n",
    "    df_smoothed[f'{col}_smooth'] = rolling_average_smoothing(df[col], window=window_size)\n",
    "\n",
    "# Fill NaN values from smoothing\n",
    "df_smoothed = df_smoothed.fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "print(f\"Applied smoothing with window size: {window_size}\")\n",
    "print(f\"Smoothed columns added: {[f'{col}_smooth' for col in sensor_columns]}\")\n",
    "\n",
    "# Visualize original vs smoothed signals\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(sensor_columns):\n",
    "    sample_data = df.iloc[1000:1200]  # Sample 200 points\n",
    "    axes[i].plot(sample_data['Timestamp'], sample_data[col], alpha=0.7, label='Original', linewidth=1)\n",
    "    axes[i].plot(sample_data['Timestamp'], df_smoothed.iloc[1000:1200][f'{col}_smooth'], \n",
    "                 label='Smoothed', linewidth=2)\n",
    "    axes[i].set_title(f'{col} - Original vs Smoothed')\n",
    "    axes[i].set_xlabel('Time')\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881cceeb",
   "metadata": {},
   "source": [
    "# 4. Time Domain Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time domain features using windowing approach\n",
    "def extract_time_domain_features(data, window_size=8, overlap_ratio=0.25):\n",
    "    \"\"\"\n",
    "    Extract time domain features from sensor data using sliding windows\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    overlap = int(window_size * overlap_ratio)\n",
    "    \n",
    "    for i in range(0, len(data) - window_size + 1, overlap):\n",
    "        window = data.iloc[i:i+window_size]\n",
    "        \n",
    "        features_dict = {\n",
    "            'window_start': i,\n",
    "            'timestamp': window['Timestamp'].iloc[-1],\n",
    "            'class': window['Class_numeric'].iloc[-1]\n",
    "        }\n",
    "        \n",
    "        # Extract features for each sensor column\n",
    "        for col in sensor_columns:\n",
    "            if col in window.columns:\n",
    "                values = window[col].values\n",
    "                features_dict.update({\n",
    "                    f'{col}_mean': np.mean(values),\n",
    "                    f'{col}_std': np.std(values),\n",
    "                    f'{col}_min': np.min(values),\n",
    "                    f'{col}_max': np.max(values),\n",
    "                    f'{col}_median': np.median(values),\n",
    "                    f'{col}_range': np.max(values) - np.min(values),\n",
    "                    f'{col}_rms': np.sqrt(np.mean(values**2)),\n",
    "                    f'{col}_var': np.var(values),\n",
    "                    f'{col}_skew': pd.Series(values).skew(),\n",
    "                    f'{col}_kurtosis': pd.Series(values).kurtosis()\n",
    "                })\n",
    "        \n",
    "        features_list.append(features_dict)\n",
    "    \n",
    "    return pd.DataFrame(features_list)\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting time domain features...\")\n",
    "time_features = extract_time_domain_features(df_smoothed, window_size=8, overlap_ratio=0.25)\n",
    "\n",
    "print(f\"Extracted features shape: {time_features.shape}\")\n",
    "print(f\"Number of features per window: {len([col for col in time_features.columns if col not in ['window_start', 'timestamp', 'class']])}\")\n",
    "print(f\"Feature columns: {[col for col in time_features.columns if '_mean' in col][:5]}...\")  # Show first 5 mean features\n",
    "\n",
    "# Display sample features\n",
    "time_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4547d432",
   "metadata": {},
   "source": [
    "# 5. Jerk Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e445a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate jerk (derivative of acceleration)\n",
    "def calculate_jerk_features(data):\n",
    "    \"\"\"Calculate jerk features from acceleration data\"\"\"\n",
    "    jerk_data = data.copy()\n",
    "    \n",
    "    # Time difference for derivative calculation\n",
    "    time_diff = data['Timestamp'].diff().fillna(0.5)  # Default to 0.5s\n",
    "    \n",
    "    # Calculate jerk for each acceleration axis\n",
    "    jerk_data['JerkX'] = data['AccX'].diff().div(time_diff, fill_value=0)\n",
    "    jerk_data['JerkY'] = data['AccY'].diff().div(time_diff, fill_value=0)\n",
    "    jerk_data['JerkZ'] = data['AccZ'].diff().div(time_diff, fill_value=0)\n",
    "    \n",
    "    # Calculate jerk magnitude\n",
    "    jerk_data['Jerk_magnitude'] = np.sqrt(\n",
    "        jerk_data['JerkX']**2 + jerk_data['JerkY']**2 + jerk_data['JerkZ']**2\n",
    "    )\n",
    "    \n",
    "    # Fill initial NaN values\n",
    "    jerk_data = jerk_data.fillna(0)\n",
    "    \n",
    "    return jerk_data\n",
    "\n",
    "# Calculate jerk features\n",
    "print(\"Calculating jerk features...\")\n",
    "df_with_jerk = calculate_jerk_features(df_smoothed)\n",
    "\n",
    "jerk_columns = ['JerkX', 'JerkY', 'JerkZ', 'Jerk_magnitude']\n",
    "print(f\"Added jerk columns: {jerk_columns}\")\n",
    "\n",
    "# Add jerk-based statistical features to time features\n",
    "def add_jerk_to_time_features(time_features, jerk_data, window_size=8, overlap_ratio=0.25):\n",
    "    \"\"\"Add jerk features to existing time domain features\"\"\"\n",
    "    overlap = int(window_size * overlap_ratio)\n",
    "    \n",
    "    for i, row in time_features.iterrows():\n",
    "        start_idx = row['window_start']\n",
    "        end_idx = start_idx + window_size\n",
    "        \n",
    "        if end_idx <= len(jerk_data):\n",
    "            window_jerk = jerk_data.iloc[start_idx:end_idx]\n",
    "            \n",
    "            for col in jerk_columns:\n",
    "                if col in window_jerk.columns:\n",
    "                    values = window_jerk[col].values\n",
    "                    time_features.loc[i, f'{col}_mean'] = np.mean(values)\n",
    "                    time_features.loc[i, f'{col}_std'] = np.std(values)\n",
    "                    time_features.loc[i, f'{col}_max'] = np.max(values)\n",
    "                    time_features.loc[i, f'{col}_rms'] = np.sqrt(np.mean(values**2))\n",
    "    \n",
    "    return time_features\n",
    "\n",
    "# Add jerk features to time domain features\n",
    "time_features = add_jerk_to_time_features(time_features, df_with_jerk)\n",
    "\n",
    "print(f\"Updated features shape: {time_features.shape}\")\n",
    "\n",
    "# Visualize jerk signals\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(['AccX', 'AccY', 'AccZ']):\n",
    "    plt.subplot(3, 2, i*2+1)\n",
    "    sample_data = df_with_jerk.iloc[1000:1200]\n",
    "    plt.plot(sample_data['Timestamp'], sample_data[col], label=f'{col}')\n",
    "    plt.title(f'Acceleration - {col}')\n",
    "    plt.ylabel('Acceleration')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(3, 2, i*2+2)\n",
    "    plt.plot(sample_data['Timestamp'], sample_data[f'Jerk{col[-1]}'], label=f'Jerk{col[-1]}', color='red')\n",
    "    plt.title(f'Jerk - {col[-1]} axis')\n",
    "    plt.ylabel('Jerk')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990bf061",
   "metadata": {},
   "source": [
    "# 6. Magnitude Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate magnitude features\n",
    "def calculate_magnitude_features(data):\n",
    "    \"\"\"Calculate magnitude features for acceleration and gyroscope\"\"\"\n",
    "    mag_data = data.copy()\n",
    "    \n",
    "    # Calculate magnitude vectors\n",
    "    mag_data['Acc_magnitude'] = np.sqrt(\n",
    "        data['AccX']**2 + data['AccY']**2 + data['AccZ']**2\n",
    "    )\n",
    "    mag_data['Gyro_magnitude'] = np.sqrt(\n",
    "        data['GyroX']**2 + data['GyroY']**2 + data['GyroZ']**2\n",
    "    )\n",
    "    \n",
    "    # Calculate additional magnitude-based features\n",
    "    mag_data['Total_magnitude'] = mag_data['Acc_magnitude'] + mag_data['Gyro_magnitude']\n",
    "    mag_data['Magnitude_ratio'] = mag_data['Acc_magnitude'] / (mag_data['Gyro_magnitude'] + 1e-8)\n",
    "    \n",
    "    return mag_data\n",
    "\n",
    "# Calculate magnitude features\n",
    "print(\"Calculating magnitude features...\")\n",
    "df_with_magnitudes = calculate_magnitude_features(df_with_jerk)\n",
    "\n",
    "magnitude_columns = ['Acc_magnitude', 'Gyro_magnitude', 'Total_magnitude', 'Magnitude_ratio']\n",
    "print(f\"Added magnitude columns: {magnitude_columns}\")\n",
    "\n",
    "# Add magnitude features to time domain features\n",
    "def add_magnitude_to_time_features(time_features, mag_data, window_size=8, overlap_ratio=0.25):\n",
    "    \"\"\"Add magnitude features to existing time domain features\"\"\"\n",
    "    overlap = int(window_size * overlap_ratio)\n",
    "    \n",
    "    for i, row in time_features.iterrows():\n",
    "        start_idx = row['window_start']\n",
    "        end_idx = start_idx + window_size\n",
    "        \n",
    "        if end_idx <= len(mag_data):\n",
    "            window_mag = mag_data.iloc[start_idx:end_idx]\n",
    "            \n",
    "            for col in magnitude_columns:\n",
    "                if col in window_mag.columns:\n",
    "                    values = window_mag[col].values\n",
    "                    time_features.loc[i, f'{col}_mean'] = np.mean(values)\n",
    "                    time_features.loc[i, f'{col}_std'] = np.std(values)\n",
    "                    time_features.loc[i, f'{col}_max'] = np.max(values)\n",
    "                    time_features.loc[i, f'{col}_min'] = np.min(values)\n",
    "                    time_features.loc[i, f'{col}_range'] = np.max(values) - np.min(values)\n",
    "    \n",
    "    return time_features\n",
    "\n",
    "# Add magnitude features\n",
    "time_features = add_magnitude_to_time_features(time_features, df_with_magnitudes)\n",
    "\n",
    "print(f\"Updated features shape: {time_features.shape}\")\n",
    "\n",
    "# Visualize magnitude features by class\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Box plots for magnitude features by class\n",
    "magnitude_cols_to_plot = ['Acc_magnitude_mean', 'Gyro_magnitude_mean', 'Total_magnitude_mean']\n",
    "\n",
    "for i, col in enumerate(magnitude_cols_to_plot):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.boxplot(data=time_features, x='class', y=col)\n",
    "    plt.title(f'{col} by Class')\n",
    "    plt.xlabel('Class (0=SLOW, 1=NORMAL, 2=AGGRESSIVE)')\n",
    "\n",
    "# Time series plots\n",
    "for i, col in enumerate(['Acc_magnitude', 'Gyro_magnitude', 'Total_magnitude']):\n",
    "    plt.subplot(2, 3, i+4)\n",
    "    sample_data = df_with_magnitudes.iloc[1000:1500]\n",
    "    classes = sample_data['Class'].unique()\n",
    "    colors = ['green', 'blue', 'red']\n",
    "    \n",
    "    for cls, color in zip(classes, colors):\n",
    "        class_data = sample_data[sample_data['Class'] == cls]\n",
    "        if len(class_data) > 0:\n",
    "            plt.plot(class_data['Timestamp'], class_data[col], \n",
    "                    alpha=0.7, color=color, label=cls)\n",
    "    \n",
    "    plt.title(f'{col} Time Series')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ad150",
   "metadata": {},
   "source": [
    "# 7. Frequency Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract frequency domain features\n",
    "def calculate_frequency_features(data, sampling_rate=2):\n",
    "    \"\"\"Calculate frequency domain features using FFT\"\"\"\n",
    "    freq_features = {}\n",
    "    \n",
    "    # Apply FFT\n",
    "    fft = np.fft.fft(data)\n",
    "    freqs = np.fft.fftfreq(len(data), 1/sampling_rate)\n",
    "    \n",
    "    # Power spectral density\n",
    "    psd = np.abs(fft)**2\n",
    "    \n",
    "    # Energy in different frequency bands\n",
    "    freq_features['total_energy'] = np.sum(psd)\n",
    "    freq_features['mean_frequency'] = np.sum(freqs[:len(freqs)//2] * psd[:len(psd)//2]) / np.sum(psd[:len(psd)//2])\n",
    "    freq_features['spectral_centroid'] = np.sum(freqs[:len(freqs)//2] * psd[:len(psd)//2]) / np.sum(psd[:len(psd)//2])\n",
    "    \n",
    "    # Frequency band energy (0-0.5Hz, 0.5-1Hz, 1-2Hz)\n",
    "    pos_freqs = freqs[:len(freqs)//2]\n",
    "    pos_psd = psd[:len(psd)//2]\n",
    "    \n",
    "    band1 = (pos_freqs >= 0) & (pos_freqs < 0.5)\n",
    "    band2 = (pos_freqs >= 0.5) & (pos_freqs < 1.0)\n",
    "    band3 = (pos_freqs >= 1.0) & (pos_freqs < 2.0)\n",
    "    \n",
    "    freq_features['energy_band_0_0.5'] = np.sum(pos_psd[band1])\n",
    "    freq_features['energy_band_0.5_1'] = np.sum(pos_psd[band2])\n",
    "    freq_features['energy_band_1_2'] = np.sum(pos_psd[band3])\n",
    "    \n",
    "    # Spectral features\n",
    "    freq_features['spectral_variance'] = np.var(pos_psd)\n",
    "    freq_features['spectral_skewness'] = pd.Series(pos_psd).skew()\n",
    "    freq_features['spectral_kurtosis'] = pd.Series(pos_psd).kurtosis()\n",
    "    \n",
    "    return freq_features\n",
    "\n",
    "# Add frequency features to time domain features\n",
    "def add_frequency_to_time_features(time_features, signal_data, window_size=8, overlap_ratio=0.25):\n",
    "    \"\"\"Add frequency domain features to existing time domain features\"\"\"\n",
    "    overlap = int(window_size * overlap_ratio)\n",
    "    \n",
    "    for i, row in time_features.iterrows():\n",
    "        start_idx = row['window_start']\n",
    "        end_idx = start_idx + window_size\n",
    "        \n",
    "        if end_idx <= len(signal_data):\n",
    "            window_data = signal_data.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Calculate frequency features for each sensor\n",
    "            for col in sensor_columns:\n",
    "                if col in window_data.columns:\n",
    "                    signal = window_data[col].values\n",
    "                    freq_features = calculate_frequency_features(signal)\n",
    "                    \n",
    "                    for freq_name, freq_value in freq_features.items():\n",
    "                        time_features.loc[i, f'{col}_{freq_name}'] = freq_value\n",
    "    \n",
    "    return time_features\n",
    "\n",
    "print(\"Calculating frequency domain features...\")\n",
    "time_features = add_frequency_to_time_features(time_features, df_with_magnitudes)\n",
    "\n",
    "print(f\"Updated features shape: {time_features.shape}\")\n",
    "\n",
    "# Visualize frequency domain analysis\n",
    "def plot_frequency_analysis(data, column, sample_start=1000, sample_length=200):\n",
    "    \"\"\"Plot time domain and frequency domain analysis\"\"\"\n",
    "    sample_data = data.iloc[sample_start:sample_start+sample_length][column].values\n",
    "    \n",
    "    # FFT\n",
    "    fft = np.fft.fft(sample_data)\n",
    "    freqs = np.fft.fftfreq(len(sample_data), 1/2)  # 2 Hz sampling rate\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Time domain\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(sample_data)\n",
    "    plt.title(f'Time Domain - {column}')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Frequency domain\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(freqs[:len(freqs)//2], np.abs(fft[:len(fft)//2]))\n",
    "    plt.title(f'Frequency Domain - {column}')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show frequency analysis for a few sensors\n",
    "for col in ['AccX', 'AccY', 'GyroX']:\n",
    "    plot_frequency_analysis(df_with_magnitudes, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad80fa",
   "metadata": {},
   "source": [
    "# 8. Feature Selection and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267345b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for machine learning\n",
    "print(\"Preparing features for machine learning...\")\n",
    "\n",
    "# Remove non-feature columns\n",
    "feature_columns = [col for col in time_features.columns \n",
    "                  if col not in ['window_start', 'timestamp', 'class']]\n",
    "\n",
    "X = time_features[feature_columns].fillna(0)\n",
    "y = time_features['class']\n",
    "\n",
    "print(f\"Total features extracted: {len(feature_columns)}\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# Handle infinite values and missing data\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Determine number of components for 95% variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Number of components for 95% variance: {n_components_95}\")\n",
    "\n",
    "# Select top features based on PCA\n",
    "n_top_features = min(50, len(feature_columns))\n",
    "feature_importance = np.abs(pca.components_[:n_components_95]).mean(axis=0)\n",
    "top_feature_indices = np.argsort(feature_importance)[-n_top_features:]\n",
    "top_features = [feature_columns[i] for i in top_feature_indices]\n",
    "\n",
    "print(f\"Selected top {n_top_features} features\")\n",
    "print(f\"Top 10 features: {top_features[-10:]}\")\n",
    "\n",
    "# Create final feature matrix\n",
    "X_final = X.iloc[:, top_feature_indices]\n",
    "X_final_scaled = X_scaled[:, top_feature_indices]\n",
    "\n",
    "# Visualize PCA results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Explained variance\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, min(51, len(pca.explained_variance_ratio_)+1)), \n",
    "         pca.explained_variance_ratio_[:50], 'bo-')\n",
    "plt.axhline(y=0.05, color='r', linestyle='--', alpha=0.7)\n",
    "plt.title('PCA Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, min(51, len(cumulative_variance)+1)), \n",
    "         cumulative_variance[:50], 'ro-')\n",
    "plt.axhline(y=0.95, color='b', linestyle='--', alpha=0.7)\n",
    "plt.axvline(x=n_components_95, color='g', linestyle='--', alpha=0.7)\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance\n",
    "plt.subplot(1, 3, 3)\n",
    "sorted_importance = np.sort(feature_importance)[-20:]\n",
    "plt.barh(range(len(sorted_importance)), sorted_importance)\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature Rank')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature correlation analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = X_final.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix (Top Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbf10f",
   "metadata": {},
   "source": [
    "# 9. Data Preparation for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation/test splits\n",
    "print(\"Creating train/validation/test splits...\")\n",
    "\n",
    "# Split data stratified by class\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_final_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "print(\"Training:\", y_train.value_counts(normalize=True).sort_index())\n",
    "print(\"Validation:\", y_val.value_counts(normalize=True).sort_index())\n",
    "print(\"Test:\", y_test.value_counts(normalize=True).sort_index())\n",
    "\n",
    "# Create data loader class for organized data management\n",
    "@dataclass\n",
    "class DataLoader:\n",
    "    X_train: np.ndarray\n",
    "    X_val: np.ndarray\n",
    "    X_test: np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_val: np.ndarray\n",
    "    y_test: np.ndarray\n",
    "    feature_names: list\n",
    "    scaler: StandardScaler\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        \"\"\"Calculate class weights for handling imbalanced data\"\"\"\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            'balanced', classes=np.unique(self.y_train), y=self.y_train\n",
    "        )\n",
    "        return {i: weight for i, weight in enumerate(class_weights)}\n",
    "    \n",
    "    def get_sample_weights(self):\n",
    "        \"\"\"Get sample weights for training\"\"\"\n",
    "        return class_weight.compute_sample_weight('balanced', self.y_train)\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(\n",
    "    X_train=X_train,\n",
    "    X_val=X_val,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_val=y_val,\n",
    "    y_test=y_test,\n",
    "    feature_names=top_features,\n",
    "    scaler=scaler\n",
    ")\n",
    "\n",
    "class_weights = data_loader.get_class_weights()\n",
    "print(f\"\\nCalculated class weights: {class_weights}\")\n",
    "\n",
    "# Visualize class distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (split_name, split_y) in enumerate([('Train', y_train), ('Validation', y_val), ('Test', y_test)]):\n",
    "    split_counts = split_y.value_counts().sort_index()\n",
    "    colors = ['green', 'blue', 'red']\n",
    "    \n",
    "    axes[i].bar(split_counts.index, split_counts.values, color=colors)\n",
    "    axes[i].set_title(f'{split_name} Set Class Distribution')\n",
    "    axes[i].set_xlabel('Class (0=SLOW, 1=NORMAL, 2=AGGRESSIVE)')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].set_xticks([0, 1, 2])\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = len(split_y)\n",
    "    for j, count in enumerate(split_counts.values):\n",
    "        axes[i].text(j, count + total*0.01, f'{count/total*100:.1f}%', \n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Data preparation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41040ee",
   "metadata": {},
   "source": [
    "# 10. Traditional Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation class\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, data_loader):\n",
    "        self.data_loader = data_loader\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def train_and_evaluate(self, model, model_name, **model_params):\n",
    "        \"\"\"Train and evaluate a model\"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        if model_name in ['SVM', 'Logistic Regression']:\n",
    "            # Use pipeline with scaling for SVM and LogReg\n",
    "            clf = make_pipeline(StandardScaler(), model(**model_params))\n",
    "        else:\n",
    "            clf = model(**model_params)\n",
    "        \n",
    "        # Train model\n",
    "        start_time = datetime.now()\n",
    "        clf.fit(self.data_loader.X_train, self.data_loader.y_train)\n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = clf.predict(self.data_loader.X_train)\n",
    "        y_pred_val = clf.predict(self.data_loader.X_val)\n",
    "        y_pred_test = clf.predict(self.data_loader.X_test)\n",
    "        \n",
    "        # Probabilities for ROC analysis\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_proba_test = clf.predict_proba(self.data_loader.X_test)\n",
    "        else:\n",
    "            y_proba_test = None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = {\n",
    "            'model': clf,\n",
    "            'training_time': training_time,\n",
    "            'train_accuracy': accuracy_score(self.data_loader.y_train, y_pred_train),\n",
    "            'val_accuracy': accuracy_score(self.data_loader.y_val, y_pred_val),\n",
    "            'test_accuracy': accuracy_score(self.data_loader.y_test, y_pred_test),\n",
    "            'test_balanced_accuracy': balanced_accuracy_score(self.data_loader.y_test, y_pred_test),\n",
    "            'test_precision': precision_score(self.data_loader.y_test, y_pred_test, average='weighted'),\n",
    "            'test_recall': recall_score(self.data_loader.y_test, y_pred_test, average='weighted'),\n",
    "            'predictions': y_pred_test,\n",
    "            'probabilities': y_proba_test,\n",
    "            'confusion_matrix': confusion_matrix(self.data_loader.y_test, y_pred_test),\n",
    "            'classification_report': classification_report(self.data_loader.y_test, y_pred_test, \n",
    "                                                         target_names=['SLOW', 'NORMAL', 'AGGRESSIVE'])\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        self.models[model_name] = clf\n",
    "        self.results[model_name] = results\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "        print(f\"Train Accuracy: {results['train_accuracy']:.4f}\")\n",
    "        print(f\"Validation Accuracy: {results['val_accuracy']:.4f}\")\n",
    "        print(f\"Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "        print(f\"Test Balanced Accuracy: {results['test_balanced_accuracy']:.4f}\")\n",
    "        print(f\"Test Precision: {results['test_precision']:.4f}\")\n",
    "        print(f\"Test Recall: {results['test_recall']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(data_loader)\n",
    "\n",
    "# Train multiple models\n",
    "models_to_train = [\n",
    "    (LogisticRegression, 'Logistic Regression', {'random_state': 42, 'max_iter': 1000}),\n",
    "    (RandomForestClassifier, 'Random Forest', {'n_estimators': 100, 'random_state': 42, 'class_weight': 'balanced'}),\n",
    "    (SVC, 'SVM', {'random_state': 42, 'class_weight': 'balanced', 'probability': True}),\n",
    "    (GaussianNB, 'Naive Bayes', {}),\n",
    "    (xgb.XGBClassifier, 'XGBoost', {'random_state': 42, 'eval_metric': 'mlogloss'})\n",
    "]\n",
    "\n",
    "# Train all models\n",
    "for model_class, model_name, params in models_to_train:\n",
    "    try:\n",
    "        evaluator.train_and_evaluate(model_class, model_name, **params)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"All models trained successfully!\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266692ca",
   "metadata": {},
   "source": [
    "# 11. Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models\n",
    "class DeepLearningEvaluator:\n",
    "    def __init__(self, data_loader):\n",
    "        self.data_loader = data_loader\n",
    "        self.models = {}\n",
    "        self.histories = {}\n",
    "        \n",
    "    def create_mlp_model(self, input_dim, hidden_layers=[64, 32, 16], dropout_rate=0.3):\n",
    "        \"\"\"Create Multi-Layer Perceptron model\"\"\"\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Input(shape=(input_dim,)))\n",
    "        \n",
    "        for i, units in enumerate(hidden_layers):\n",
    "            model.add(keras.layers.Dense(units, activation='relu'))\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "            if dropout_rate > 0:\n",
    "                model.add(keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "        model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_neural_network(self, model, model_name, epochs=100, batch_size=32, \n",
    "                           early_stopping_patience=10):\n",
    "        \"\"\"Train neural network model\"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Calculate class weights\n",
    "        class_weights = self.data_loader.get_class_weights()\n",
    "        \n",
    "        # Train model\n",
    "        start_time = datetime.now()\n",
    "        history = model.fit(\n",
    "            self.data_loader.X_train, self.data_loader.y_train,\n",
    "            validation_data=(self.data_loader.X_val, self.data_loader.y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Evaluate model\n",
    "        test_loss, test_accuracy, test_precision, test_recall = model.evaluate(\n",
    "            self.data_loader.X_test, self.data_loader.y_test, verbose=0\n",
    "        )\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_proba = model.predict(self.data_loader.X_test, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        \n",
    "        # Store results\n",
    "        results = {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'training_time': training_time,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_precision': test_precision,\n",
    "            'test_recall': test_recall,\n",
    "            'test_balanced_accuracy': balanced_accuracy_score(self.data_loader.y_test, y_pred),\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba,\n",
    "            'confusion_matrix': confusion_matrix(self.data_loader.y_test, y_pred),\n",
    "            'classification_report': classification_report(\n",
    "                self.data_loader.y_test, y_pred, \n",
    "                target_names=['SLOW', 'NORMAL', 'AGGRESSIVE']\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.models[model_name] = model\n",
    "        self.histories[model_name] = history\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Test Precision: {test_precision:.4f}\")\n",
    "        print(f\"Test Recall: {test_recall:.4f}\")\n",
    "        print(f\"Test Balanced Accuracy: {results['test_balanced_accuracy']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_training_history(self, model_name):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if model_name not in self.histories:\n",
    "            print(f\"No history found for {model_name}\")\n",
    "            return\n",
    "        \n",
    "        history = self.histories[model_name]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        axes[0, 0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "        axes[0, 0].set_title('Model Accuracy')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss\n",
    "        axes[0, 1].plot(history.history['loss'], label='Train Loss')\n",
    "        axes[0, 1].plot(history.history['val_loss'], label='Val Loss')\n",
    "        axes[0, 1].set_title('Model Loss')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Precision\n",
    "        axes[1, 0].plot(history.history['precision'], label='Train Precision')\n",
    "        axes[1, 0].plot(history.history['val_precision'], label='Val Precision')\n",
    "        axes[1, 0].set_title('Model Precision')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Precision')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Recall\n",
    "        axes[1, 1].plot(history.history['recall'], label='Train Recall')\n",
    "        axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n",
    "        axes[1, 1].set_title('Model Recall')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Recall')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize deep learning evaluator\n",
    "dl_evaluator = DeepLearningEvaluator(data_loader)\n",
    "\n",
    "# Create and train MLP models\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Simple MLP\n",
    "simple_mlp = dl_evaluator.create_mlp_model(\n",
    "    input_dim=input_dim,\n",
    "    hidden_layers=[32, 16],\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "\n",
    "simple_mlp_results = dl_evaluator.train_neural_network(\n",
    "    simple_mlp, 'Simple MLP', epochs=50, batch_size=32\n",
    ")\n",
    "\n",
    "# Complex MLP\n",
    "complex_mlp = dl_evaluator.create_mlp_model(\n",
    "    input_dim=input_dim,\n",
    "    hidden_layers=[128, 64, 32, 16],\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "complex_mlp_results = dl_evaluator.train_neural_network(\n",
    "    complex_mlp, 'Complex MLP', epochs=50, batch_size=32\n",
    ")\n",
    "\n",
    "# Plot training histories\n",
    "print(\"Plotting training histories...\")\n",
    "dl_evaluator.plot_training_history('Simple MLP')\n",
    "dl_evaluator.plot_training_history('Complex MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e83f6",
   "metadata": {},
   "source": [
    "# 12. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c320f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison\n",
    "def create_model_comparison():\n",
    "    \"\"\"Create comprehensive model comparison\"\"\"\n",
    "    \n",
    "    # Combine traditional ML and DL results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Add traditional ML results\n",
    "    for model_name, results in evaluator.results.items():\n",
    "        all_results[model_name] = {\n",
    "            'Test Accuracy': results['test_accuracy'],\n",
    "            'Test Balanced Accuracy': results['test_balanced_accuracy'],\n",
    "            'Test Precision': results['test_precision'],\n",
    "            'Test Recall': results['test_recall'],\n",
    "            'Training Time': results['training_time']\n",
    "        }\n",
    "    \n",
    "    # Add deep learning results\n",
    "    dl_results = {\n",
    "        'Simple MLP': simple_mlp_results,\n",
    "        'Complex MLP': complex_mlp_results\n",
    "    }\n",
    "    \n",
    "    for model_name, results in dl_results.items():\n",
    "        all_results[model_name] = {\n",
    "            'Test Accuracy': results['test_accuracy'],\n",
    "            'Test Balanced Accuracy': results['test_balanced_accuracy'],\n",
    "            'Test Precision': results['test_precision'],\n",
    "            'Test Recall': results['test_recall'],\n",
    "            'Training Time': results['training_time']\n",
    "        }\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(all_results).T\n",
    "    comparison_df = comparison_df.round(4)\n",
    "    \n",
    "    return comparison_df, all_results\n",
    "\n",
    "# Create model comparison\n",
    "comparison_df, all_results = create_model_comparison()\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.sort_values('Test Accuracy', ascending=False))\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 0].bar(comparison_df.index, comparison_df['Test Accuracy'], color='skyblue')\n",
    "axes[0, 0].set_title('Test Accuracy Comparison')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Balanced accuracy comparison\n",
    "axes[0, 1].bar(comparison_df.index, comparison_df['Test Balanced Accuracy'], color='lightgreen')\n",
    "axes[0, 1].set_title('Test Balanced Accuracy Comparison')\n",
    "axes[0, 1].set_ylabel('Balanced Accuracy')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision comparison\n",
    "axes[1, 0].bar(comparison_df.index, comparison_df['Test Precision'], color='orange')\n",
    "axes[1, 0].set_title('Test Precision Comparison')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "axes[1, 1].bar(comparison_df.index, comparison_df['Training Time'], color='salmon')\n",
    "axes[1, 1].set_title('Training Time Comparison')\n",
    "axes[1, 1].set_ylabel('Time (seconds)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices for best models\n",
    "best_models = comparison_df.sort_values('Test Accuracy', ascending=False).head(3).index\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, model_name in enumerate(best_models):\n",
    "    if model_name in evaluator.results:\n",
    "        cm = evaluator.results[model_name]['confusion_matrix']\n",
    "    else:\n",
    "        # For deep learning models\n",
    "        if model_name == 'Simple MLP':\n",
    "            cm = simple_mlp_results['confusion_matrix']\n",
    "        elif model_name == 'Complex MLP':\n",
    "            cm = complex_mlp_results['confusion_matrix']\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['SLOW', 'NORMAL', 'AGGRESSIVE'],\n",
    "                yticklabels=['SLOW', 'NORMAL', 'AGGRESSIVE'],\n",
    "                ax=axes[i])\n",
    "    axes[i].set_title(f'{model_name} Confusion Matrix')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.sort_values('Test Accuracy', ascending=False).index[0]\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {comparison_df.loc[best_model_name, 'Test Accuracy']:.4f}\")\n",
    "print(f\"Test Balanced Accuracy: {comparison_df.loc[best_model_name, 'Test Balanced Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b00c5c",
   "metadata": {},
   "source": [
    "# 13. Behavior Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Behavior Score Calculation\n",
    "class BehaviorScoreCalculator:\n",
    "    def __init__(self, best_model, best_model_name, feature_importance=None):\n",
    "        self.best_model = best_model\n",
    "        self.best_model_name = best_model_name\n",
    "        self.feature_importance = feature_importance\n",
    "        \n",
    "    def calculate_base_score(self, predictions, probabilities):\n",
    "        \"\"\"Calculate base behavior score from model predictions\"\"\"\n",
    "        base_scores = []\n",
    "        \n",
    "        for pred, prob in zip(predictions, probabilities):\n",
    "            if hasattr(prob, '__len__') and len(prob) == 3:\n",
    "                # Use probability distribution for more nuanced scoring\n",
    "                # SLOW=0 (good), NORMAL=1 (average), AGGRESSIVE=2 (bad)\n",
    "                prob_slow, prob_normal, prob_aggressive = prob\n",
    "                \n",
    "                # Base score calculation: higher score = better behavior\n",
    "                # Scale from 0-100 where 100 is safest\n",
    "                base_score = (\n",
    "                    prob_slow * 100 +           # Full points for safe driving\n",
    "                    prob_normal * 70 +          # Moderate points for normal driving\n",
    "                    prob_aggressive * 20        # Low points for aggressive driving\n",
    "                )\n",
    "            else:\n",
    "                # Fallback for models without probabilities\n",
    "                if pred == 0:      # SLOW (safe)\n",
    "                    base_score = 85\n",
    "                elif pred == 1:    # NORMAL\n",
    "                    base_score = 70\n",
    "                else:              # AGGRESSIVE\n",
    "                    base_score = 30\n",
    "            \n",
    "            base_scores.append(base_score)\n",
    "        \n",
    "        return np.array(base_scores)\n",
    "    \n",
    "    def calculate_feature_based_adjustments(self, features, feature_names):\n",
    "        \"\"\"Calculate adjustments based on specific feature values\"\"\"\n",
    "        adjustments = np.zeros(len(features))\n",
    "        \n",
    "        # Define critical features and their impact\n",
    "        critical_features = {\n",
    "            'jerk': ['Jerk', 'jerk'],\n",
    "            'acceleration': ['Acc', 'acc'],\n",
    "            'gyroscope': ['Gyro', 'gyro'],\n",
    "            'magnitude': ['magnitude', 'Magnitude']\n",
    "        }\n",
    "        \n",
    "        for i, feature_row in enumerate(features):\n",
    "            adjustment = 0\n",
    "            \n",
    "            for j, (feature_name, feature_value) in enumerate(zip(feature_names, feature_row)):\n",
    "                # Normalize feature value (assuming standardized features)\n",
    "                normalized_value = abs(feature_value)\n",
    "                \n",
    "                # Apply penalties for extreme values\n",
    "                if any(keyword in feature_name for keyword in critical_features['jerk']):\n",
    "                    if normalized_value > 2:  # High jerk\n",
    "                        adjustment -= 5\n",
    "                    elif normalized_value > 1.5:\n",
    "                        adjustment -= 2\n",
    "                        \n",
    "                elif any(keyword in feature_name for keyword in critical_features['acceleration']):\n",
    "                    if normalized_value > 2.5:  # High acceleration\n",
    "                        adjustment -= 3\n",
    "                    elif normalized_value > 2:\n",
    "                        adjustment -= 1\n",
    "                        \n",
    "                elif any(keyword in feature_name for keyword in critical_features['magnitude']):\n",
    "                    if normalized_value > 2:  # High overall magnitude\n",
    "                        adjustment -= 2\n",
    "            \n",
    "            adjustments[i] = max(adjustment, -20)  # Cap negative adjustment\n",
    "        \n",
    "        return adjustments\n",
    "    \n",
    "    def calculate_confidence_intervals(self, base_scores, model_confidence):\n",
    "        \"\"\"Calculate confidence intervals for behavior scores\"\"\"\n",
    "        confidence_intervals = []\n",
    "        \n",
    "        for score, confidence in zip(base_scores, model_confidence):\n",
    "            # Higher model confidence = narrower interval\n",
    "            interval_width = (1 - confidence) * 10  # Max ±10 points\n",
    "            lower_bound = max(0, score - interval_width)\n",
    "            upper_bound = min(100, score + interval_width)\n",
    "            \n",
    "            confidence_intervals.append({\n",
    "                'lower': lower_bound,\n",
    "                'upper': upper_bound,\n",
    "                'width': interval_width\n",
    "            })\n",
    "        \n",
    "        return confidence_intervals\n",
    "    \n",
    "    def calculate_comprehensive_score(self, X_test, y_test):\n",
    "        \"\"\"Calculate comprehensive behavior scores\"\"\"\n",
    "        print(\"Calculating comprehensive behavior scores...\")\n",
    "        \n",
    "        # Get model predictions and probabilities\n",
    "        predictions = self.best_model.predict(X_test)\n",
    "        \n",
    "        if hasattr(self.best_model, 'predict_proba'):\n",
    "            probabilities = self.best_model.predict_proba(X_test)\n",
    "            model_confidence = np.max(probabilities, axis=1)\n",
    "        else:\n",
    "            # For models without predict_proba, create pseudo-probabilities\n",
    "            probabilities = np.zeros((len(predictions), 3))\n",
    "            for i, pred in enumerate(predictions):\n",
    "                probabilities[i, pred] = 0.8  # Assume 80% confidence\n",
    "                # Distribute remaining probability\n",
    "                remaining = (1 - 0.8) / 2\n",
    "                for j in range(3):\n",
    "                    if j != pred:\n",
    "                        probabilities[i, j] = remaining\n",
    "            model_confidence = np.full(len(predictions), 0.8)\n",
    "        \n",
    "        # Calculate base scores\n",
    "        base_scores = self.calculate_base_score(predictions, probabilities)\n",
    "        \n",
    "        # Calculate feature-based adjustments\n",
    "        feature_adjustments = self.calculate_feature_based_adjustments(\n",
    "            X_test, data_loader.feature_names\n",
    "        )\n",
    "        \n",
    "        # Calculate final scores\n",
    "        final_scores = np.clip(base_scores + feature_adjustments, 0, 100)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        confidence_intervals = self.calculate_confidence_intervals(\n",
    "            final_scores, model_confidence\n",
    "        )\n",
    "        \n",
    "        # Create risk categories\n",
    "        risk_categories = []\n",
    "        for score in final_scores:\n",
    "            if score >= 80:\n",
    "                risk_categories.append('LOW_RISK')\n",
    "            elif score >= 60:\n",
    "                risk_categories.append('MEDIUM_RISK')\n",
    "            elif score >= 40:\n",
    "                risk_categories.append('HIGH_RISK')\n",
    "            else:\n",
    "                risk_categories.append('VERY_HIGH_RISK')\n",
    "        \n",
    "        return {\n",
    "            'final_scores': final_scores,\n",
    "            'base_scores': base_scores,\n",
    "            'feature_adjustments': feature_adjustments,\n",
    "            'model_confidence': model_confidence,\n",
    "            'confidence_intervals': confidence_intervals,\n",
    "            'risk_categories': risk_categories,\n",
    "            'predictions': predictions,\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "\n",
    "# Get the best model for scoring\n",
    "if best_model_name in evaluator.models:\n",
    "    best_model = evaluator.models[best_model_name]\n",
    "elif best_model_name == 'Simple MLP':\n",
    "    best_model = simple_mlp\n",
    "elif best_model_name == 'Complex MLP':\n",
    "    best_model = complex_mlp\n",
    "else:\n",
    "    best_model = evaluator.models[list(evaluator.models.keys())[0]]  # Fallback\n",
    "\n",
    "# Initialize behavior score calculator\n",
    "behavior_calculator = BehaviorScoreCalculator(best_model, best_model_name)\n",
    "\n",
    "# Calculate behavior scores\n",
    "behavior_scores = behavior_calculator.calculate_comprehensive_score(\n",
    "    data_loader.X_test, data_loader.y_test\n",
    ")\n",
    "\n",
    "print(\"Behavior score calculation completed!\")\n",
    "print(f\"Score statistics:\")\n",
    "print(f\"Mean score: {np.mean(behavior_scores['final_scores']):.2f}\")\n",
    "print(f\"Std score: {np.std(behavior_scores['final_scores']):.2f}\")\n",
    "print(f\"Min score: {np.min(behavior_scores['final_scores']):.2f}\")\n",
    "print(f\"Max score: {np.max(behavior_scores['final_scores']):.2f}\")\n",
    "\n",
    "# Visualize behavior scores\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Score distribution\n",
    "axes[0, 0].hist(behavior_scores['final_scores'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Behavior Score Distribution')\n",
    "axes[0, 0].set_xlabel('Behavior Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(np.mean(behavior_scores['final_scores']), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scores by actual class\n",
    "class_names = ['SLOW', 'NORMAL', 'AGGRESSIVE']\n",
    "score_by_class = [behavior_scores['final_scores'][data_loader.y_test == i] for i in range(3)]\n",
    "axes[0, 1].boxplot(score_by_class, labels=class_names)\n",
    "axes[0, 1].set_title('Behavior Scores by Actual Class')\n",
    "axes[0, 1].set_ylabel('Behavior Score')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Risk category distribution\n",
    "risk_counts = pd.Series(behavior_scores['risk_categories']).value_counts()\n",
    "axes[1, 0].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Risk Category Distribution')\n",
    "\n",
    "# Score vs Confidence\n",
    "axes[1, 1].scatter(behavior_scores['model_confidence'], behavior_scores['final_scores'], \n",
    "                  alpha=0.6, c=data_loader.y_test, cmap='viridis')\n",
    "axes[1, 1].set_title('Behavior Score vs Model Confidence')\n",
    "axes[1, 1].set_xlabel('Model Confidence')\n",
    "axes[1, 1].set_ylabel('Behavior Score')\n",
    "axes[1, 1].colorbar = plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251ef3b",
   "metadata": {},
   "source": [
    "# 14. Save Combined DataFrame with Behavior Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b3a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive combined DataFrame with all features and behavior scores\n",
    "print(\"Creating comprehensive combined DataFrame...\")\n",
    "\n",
    "# Reconstruct original data indices for proper alignment\n",
    "total_samples = len(data_loader.X_train) + len(data_loader.X_test)\n",
    "train_indices = range(len(data_loader.X_train))\n",
    "test_indices = range(len(data_loader.X_train), total_samples)\n",
    "\n",
    "# Create combined feature DataFrame\n",
    "combined_features = pd.DataFrame(\n",
    "    np.vstack([data_loader.X_train, data_loader.X_test]),\n",
    "    columns=data_loader.feature_names\n",
    ")\n",
    "\n",
    "# Create combined labels\n",
    "combined_labels = np.hstack([data_loader.y_train, data_loader.y_test])\n",
    "\n",
    "# Add metadata columns\n",
    "combined_df = combined_features.copy()\n",
    "combined_df['actual_class'] = combined_labels\n",
    "combined_df['actual_class_name'] = [data_loader.label_encoder.classes_[i] for i in combined_labels]\n",
    "\n",
    "# Initialize score columns with NaN\n",
    "combined_df['behavior_score'] = np.nan\n",
    "combined_df['base_score'] = np.nan\n",
    "combined_df['feature_adjustment'] = np.nan\n",
    "combined_df['model_confidence'] = np.nan\n",
    "combined_df['predicted_class'] = np.nan\n",
    "combined_df['predicted_class_name'] = ''\n",
    "combined_df['risk_category'] = ''\n",
    "combined_df['confidence_lower'] = np.nan\n",
    "combined_df['confidence_upper'] = np.nan\n",
    "combined_df['data_split'] = ''\n",
    "\n",
    "# Mark train/test split\n",
    "combined_df.loc[train_indices, 'data_split'] = 'train'\n",
    "combined_df.loc[test_indices, 'data_split'] = 'test'\n",
    "\n",
    "# Fill in behavior scores for test data\n",
    "test_mask = combined_df['data_split'] == 'test'\n",
    "combined_df.loc[test_mask, 'behavior_score'] = behavior_scores['final_scores']\n",
    "combined_df.loc[test_mask, 'base_score'] = behavior_scores['base_scores']\n",
    "combined_df.loc[test_mask, 'feature_adjustment'] = behavior_scores['feature_adjustments']\n",
    "combined_df.loc[test_mask, 'model_confidence'] = behavior_scores['model_confidence']\n",
    "combined_df.loc[test_mask, 'predicted_class'] = behavior_scores['predictions']\n",
    "combined_df.loc[test_mask, 'predicted_class_name'] = [\n",
    "    data_loader.label_encoder.classes_[i] for i in behavior_scores['predictions']\n",
    "]\n",
    "combined_df.loc[test_mask, 'risk_category'] = behavior_scores['risk_categories']\n",
    "\n",
    "# Add confidence intervals\n",
    "for i, interval in enumerate(behavior_scores['confidence_intervals']):\n",
    "    idx = combined_df.index[test_mask][i]\n",
    "    combined_df.loc[idx, 'confidence_lower'] = interval['lower']\n",
    "    combined_df.loc[idx, 'confidence_upper'] = interval['upper']\n",
    "\n",
    "# Calculate behavior scores for training data (for completeness)\n",
    "print(\"Calculating behavior scores for training data...\")\n",
    "train_predictions = best_model.predict(data_loader.X_train)\n",
    "if hasattr(best_model, 'predict_proba'):\n",
    "    train_probabilities = best_model.predict_proba(data_loader.X_train)\n",
    "    train_confidence = np.max(train_probabilities, axis=1)\n",
    "else:\n",
    "    train_probabilities = np.zeros((len(train_predictions), 3))\n",
    "    for i, pred in enumerate(train_predictions):\n",
    "        train_probabilities[i, pred] = 0.8\n",
    "    train_confidence = np.full(len(train_predictions), 0.8)\n",
    "\n",
    "# Calculate scores for training data\n",
    "train_base_scores = behavior_calculator.calculate_base_score(train_predictions, train_probabilities)\n",
    "train_adjustments = behavior_calculator.calculate_feature_based_adjustments(\n",
    "    data_loader.X_train, data_loader.feature_names\n",
    ")\n",
    "train_final_scores = np.clip(train_base_scores + train_adjustments, 0, 100)\n",
    "train_confidence_intervals = behavior_calculator.calculate_confidence_intervals(\n",
    "    train_final_scores, train_confidence\n",
    ")\n",
    "\n",
    "train_risk_categories = []\n",
    "for score in train_final_scores:\n",
    "    if score >= 80:\n",
    "        train_risk_categories.append('LOW_RISK')\n",
    "    elif score >= 60:\n",
    "        train_risk_categories.append('MEDIUM_RISK')\n",
    "    elif score >= 40:\n",
    "        train_risk_categories.append('HIGH_RISK')\n",
    "    else:\n",
    "        train_risk_categories.append('VERY_HIGH_RISK')\n",
    "\n",
    "# Fill in training data scores\n",
    "train_mask = combined_df['data_split'] == 'train'\n",
    "combined_df.loc[train_mask, 'behavior_score'] = train_final_scores\n",
    "combined_df.loc[train_mask, 'base_score'] = train_base_scores\n",
    "combined_df.loc[train_mask, 'feature_adjustment'] = train_adjustments\n",
    "combined_df.loc[train_mask, 'model_confidence'] = train_confidence\n",
    "combined_df.loc[train_mask, 'predicted_class'] = train_predictions\n",
    "combined_df.loc[train_mask, 'predicted_class_name'] = [\n",
    "    data_loader.label_encoder.classes_[i] for i in train_predictions\n",
    "]\n",
    "combined_df.loc[train_mask, 'risk_category'] = train_risk_categories\n",
    "\n",
    "for i, interval in enumerate(train_confidence_intervals):\n",
    "    idx = combined_df.index[train_mask][i]\n",
    "    combined_df.loc[idx, 'confidence_lower'] = interval['lower']\n",
    "    combined_df.loc[idx, 'confidence_upper'] = interval['upper']\n",
    "\n",
    "# Add model metadata\n",
    "combined_df['model_used'] = best_model_name\n",
    "combined_df['timestamp'] = pd.Timestamp.now()\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nCombined DataFrame Summary:\")\n",
    "print(f\"Total samples: {len(combined_df)}\")\n",
    "print(f\"Features: {len(data_loader.feature_names)}\")\n",
    "print(f\"Training samples: {len(combined_df[combined_df['data_split'] == 'train'])}\")\n",
    "print(f\"Test samples: {len(combined_df[combined_df['data_split'] == 'test'])}\")\n",
    "print(f\"\\nBehavior Score Statistics:\")\n",
    "print(combined_df['behavior_score'].describe())\n",
    "\n",
    "print(f\"\\nRisk Category Distribution:\")\n",
    "print(combined_df['risk_category'].value_counts())\n",
    "\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(combined_df['actual_class_name'].value_counts())\n",
    "\n",
    "# Display sample of the combined DataFrame\n",
    "print(f\"\\nSample of Combined DataFrame:\")\n",
    "display_columns = ['actual_class_name', 'predicted_class_name', 'behavior_score', \n",
    "                  'risk_category', 'model_confidence', 'data_split']\n",
    "print(combined_df[display_columns].head(10))\n",
    "\n",
    "# Save the combined DataFrame\n",
    "output_file = 'combined_telematics_data_with_behavior_scores.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✅ Combined DataFrame saved to: {output_file}\")\n",
    "\n",
    "# Save a summary report\n",
    "summary_report = f\"\"\"\n",
    "Telematics Behavior Analysis Summary Report\n",
    "=========================================\n",
    "Generated: {pd.Timestamp.now()}\n",
    "Model Used: {best_model_name}\n",
    "\n",
    "Dataset Statistics:\n",
    "- Total Samples: {len(combined_df):,}\n",
    "- Training Samples: {len(combined_df[combined_df['data_split'] == 'train']):,}\n",
    "- Test Samples: {len(combined_df[combined_df['data_split'] == 'test']):,}\n",
    "- Features: {len(data_loader.feature_names)}\n",
    "\n",
    "Behavior Score Statistics:\n",
    "- Mean Score: {combined_df['behavior_score'].mean():.2f}\n",
    "- Std Score: {combined_df['behavior_score'].std():.2f}\n",
    "- Min Score: {combined_df['behavior_score'].min():.2f}\n",
    "- Max Score: {combined_df['behavior_score'].max():.2f}\n",
    "\n",
    "Risk Category Distribution:\n",
    "{combined_df['risk_category'].value_counts().to_string()}\n",
    "\n",
    "Class Distribution:\n",
    "{combined_df['actual_class_name'].value_counts().to_string()}\n",
    "\n",
    "Model Performance:\n",
    "- Best Model: {best_model_name}\n",
    "- Test Accuracy: {evaluator.results.loc[evaluator.results['Model'] == best_model_name, 'Accuracy'].iloc[0]:.4f}\n",
    "\n",
    "Files Generated:\n",
    "1. combined_telematics_data_with_behavior_scores.csv - Complete dataset with scores\n",
    "2. telematics_analysis_summary.txt - This summary report\n",
    "\"\"\"\n",
    "\n",
    "with open('telematics_analysis_summary.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"📊 Summary report saved to: telematics_analysis_summary.txt\")\n",
    "print(\"\\n Comprehensive telematics analysis completed!\")\n",
    "print(\"The combined notebook successfully integrates:\")\n",
    "print(\"✓ Feature engineering from both source notebooks\")\n",
    "print(\"✓ Multiple machine learning models\")\n",
    "print(\"✓ Comprehensive behavior scoring system\")\n",
    "print(\"✓ Risk categorization for insurance applications\")\n",
    "print(\"✓ Complete dataset with scores for further analysis\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
