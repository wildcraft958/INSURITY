{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05c337c",
   "metadata": {},
   "source": [
    "# Gating Model - Expert Ensemble\n",
    "Reference: how-can-we-prevent-road-rage (merging outputs)\n",
    "\n",
    "This notebook implements the gating model that combines outputs from all expert models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f1d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Gating Model - Expert Ensemble - Ready for implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5b2c0",
   "metadata": {},
   "source": [
    "## Expert Model Integration\n",
    "\n",
    "Combine outputs from:\n",
    "1. Behavior Expert\n",
    "2. Geographic Expert\n",
    "3. Contextual Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertEnsemble:\n",
    "    \"\"\"\n",
    "    Ensemble model combining expert outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.expert_weights = {\n",
    "            'behavior': 0.4,\n",
    "            'geographic': 0.3,\n",
    "            'contextual': 0.3\n",
    "        }\n",
    "        \n",
    "        self.risk_thresholds = {\n",
    "            'low': 30,\n",
    "            'moderate': 60,\n",
    "            'high': 80\n",
    "        }\n",
    "    \n",
    "    def combine_expert_scores(self, behavior_score, geo_risk, context_risk):\n",
    "        \"\"\"\n",
    "        Weighted combination of expert scores\n",
    "        \"\"\"\n",
    "        # Convert scores to same scale (risk scores)\n",
    "        behavior_risk = 100 - behavior_score  # Invert behavior score\n",
    "        \n",
    "        # Weighted average\n",
    "        combined_risk = (\n",
    "            behavior_risk * self.expert_weights['behavior'] +\n",
    "            geo_risk * self.expert_weights['geographic'] +\n",
    "            context_risk * self.expert_weights['contextual']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'combined_risk': combined_risk,\n",
    "            'risk_category': self._categorize_risk(combined_risk),\n",
    "            'expert_contributions': {\n",
    "                'behavior': behavior_risk,\n",
    "                'geographic': geo_risk,\n",
    "                'contextual': context_risk\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _categorize_risk(self, risk_score):\n",
    "        \"\"\"Categorize risk level\"\"\"\n",
    "        if risk_score < self.risk_thresholds['low']:\n",
    "            return \"Low Risk\"\n",
    "        elif risk_score < self.risk_thresholds['moderate']:\n",
    "            return \"Moderate Risk\"\n",
    "        elif risk_score < self.risk_thresholds['high']:\n",
    "            return \"High Risk\"\n",
    "        else:\n",
    "            return \"Very High Risk\"\n",
    "\n",
    "# Test the ensemble\n",
    "ensemble = ExpertEnsemble()\n",
    "test_result = ensemble.combine_expert_scores(\n",
    "    behavior_score=85,\n",
    "    geo_risk=45,\n",
    "    context_risk=55\n",
    ")\n",
    "print(f\"Ensemble test result: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f6abb",
   "metadata": {},
   "source": [
    "## Premium Calculation\n",
    "\n",
    "Convert risk scores to insurance premium adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e51db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_premium_adjustment(risk_score, base_premium=1000):\n",
    "    \"\"\"\n",
    "    Calculate insurance premium based on combined risk score\n",
    "    \"\"\"\n",
    "    # Premium adjustment curve\n",
    "    if risk_score < 30:\n",
    "        adjustment_factor = 0.8  # 20% discount\n",
    "        tier = \"Preferred\"\n",
    "    elif risk_score < 50:\n",
    "        adjustment_factor = 0.9  # 10% discount\n",
    "        tier = \"Standard Plus\"\n",
    "    elif risk_score < 70:\n",
    "        adjustment_factor = 1.0  # Standard rate\n",
    "        tier = \"Standard\"\n",
    "    elif risk_score < 85:\n",
    "        adjustment_factor = 1.2  # 20% surcharge\n",
    "        tier = \"Substandard\"\n",
    "    else:\n",
    "        adjustment_factor = 1.5  # 50% surcharge\n",
    "        tier = \"High Risk\"\n",
    "    \n",
    "    adjusted_premium = base_premium * adjustment_factor\n",
    "    savings = base_premium - adjusted_premium\n",
    "    \n",
    "    return {\n",
    "        'base_premium': base_premium,\n",
    "        'adjusted_premium': adjusted_premium,\n",
    "        'adjustment_factor': adjustment_factor,\n",
    "        'savings': savings,\n",
    "        'tier': tier\n",
    "    }\n",
    "\n",
    "# Test premium calculation\n",
    "premium_result = calculate_premium_adjustment(test_result['combined_risk'])\n",
    "print(f\"Premium calculation: {premium_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b4c87",
   "metadata": {},
   "source": [
    "## Model Performance Analysis\n",
    "\n",
    "Analyze expert model contributions and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f1f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_expert_contributions(expert_scores_df):\n",
    "    \"\"\"\n",
    "    Analyze how each expert model contributes to final scores\n",
    "    \"\"\"\n",
    "    # Calculate correlations between expert scores\n",
    "    correlations = expert_scores_df[['behavior', 'geographic', 'contextual']].corr()\n",
    "    \n",
    "    # Feature importance (simplified)\n",
    "    importance = {\n",
    "        'behavior': expert_scores_df['behavior'].std(),\n",
    "        'geographic': expert_scores_df['geographic'].std(),\n",
    "        'contextual': expert_scores_df['contextual'].std()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'correlations': correlations,\n",
    "        'variability': importance\n",
    "    }\n",
    "\n",
    "# Create sample data for analysis\n",
    "sample_data = pd.DataFrame({\n",
    "    'behavior': np.random.normal(75, 15, 100),\n",
    "    'geographic': np.random.normal(50, 20, 100),\n",
    "    'contextual': np.random.normal(45, 18, 100)\n",
    "})\n",
    "\n",
    "analysis_result = analyze_expert_contributions(sample_data)\n",
    "print(\"Expert contribution analysis completed\")\n",
    "print(f\"Correlations:\\n{analysis_result['correlations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95309f84",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize expert model outputs and ensemble results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f105fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expert_distributions(expert_scores_df):\n",
    "    \"\"\"\n",
    "    Plot distributions of expert model scores\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Individual expert distributions\n",
    "    expert_scores_df['behavior'].hist(ax=axes[0,0], bins=20, alpha=0.7)\n",
    "    axes[0,0].set_title('Behavior Scores')\n",
    "    \n",
    "    expert_scores_df['geographic'].hist(ax=axes[0,1], bins=20, alpha=0.7)\n",
    "    axes[0,1].set_title('Geographic Risk Scores')\n",
    "    \n",
    "    expert_scores_df['contextual'].hist(ax=axes[1,0], bins=20, alpha=0.7)\n",
    "    axes[1,0].set_title('Contextual Risk Scores')\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    sns.heatmap(expert_scores_df[['behavior', 'geographic', 'contextual']].corr(), \n",
    "                annot=True, ax=axes[1,1], cmap='coolwarm', center=0)\n",
    "    axes[1,1].set_title('Expert Score Correlations')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Expert distribution plots created\")\n",
    "\n",
    "# Create visualization\n",
    "plot_expert_distributions(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda01cce",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "\n",
    "Optimize expert weights based on performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_expert_weights(expert_scores, actual_claims):\n",
    "    \"\"\"\n",
    "    Optimize expert weights based on claim prediction accuracy\n",
    "    \"\"\"\n",
    "    # This would be implemented with actual claims data\n",
    "    # Using optimization algorithms to find best weights\n",
    "    \n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    def objective(weights):\n",
    "        # Calculate combined scores with given weights\n",
    "        combined_scores = (\n",
    "            expert_scores['behavior'] * weights[0] +\n",
    "            expert_scores['geographic'] * weights[1] +\n",
    "            expert_scores['contextual'] * weights[2]\n",
    "        )\n",
    "        \n",
    "        # Return negative correlation (to maximize)\n",
    "        correlation = np.corrcoef(combined_scores, actual_claims)[0, 1]\n",
    "        return -correlation if not np.isnan(correlation) else 1\n",
    "    \n",
    "    # Constraints: weights sum to 1\n",
    "    constraints = {'type': 'eq', 'fun': lambda x: sum(x) - 1}\n",
    "    bounds = [(0, 1), (0, 1), (0, 1)]\n",
    "    \n",
    "    # Initial weights\n",
    "    initial_weights = [0.4, 0.3, 0.3]\n",
    "    \n",
    "    # Note: This is a placeholder - would use real claims data\n",
    "    print(\"Weight optimization framework ready (requires real claims data)\")\n",
    "    \n",
    "    return initial_weights\n",
    "\n",
    "# Test optimization framework\n",
    "optimized_weights = optimize_expert_weights(sample_data, np.random.random(100))\n",
    "print(f\"Optimized weights: {optimized_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737eb614",
   "metadata": {},
   "source": [
    "## Comprehensive CSV-Based Gating Implementation\n",
    "\n",
    "Implementation of production-ready gating mechanism that combines CSV outputs from all three expert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5869e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.12' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class ComprehensiveGatingMechanism:\n",
    "    \"\"\"\n",
    "    Production-ready gating mechanism that combines CSV outputs from expert models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, expert_weights=None, feature_importance_weights=None):\n",
    "        # Default expert weights based on domain knowledge\n",
    "        self.expert_weights = expert_weights or {\n",
    "            'behavior': 0.45,      # Highest weight - most predictive\n",
    "            'geographic': 0.30,    # Medium weight - location matters\n",
    "            'contextual': 0.25     # Lower weight - situational factors\n",
    "        }\n",
    "        \n",
    "        # Feature importance weights for final scoring\n",
    "        self.feature_importance_weights = feature_importance_weights or {\n",
    "            'acceleration_features': 0.25,\n",
    "            'driving_patterns': 0.20,\n",
    "            'location_risk': 0.20,\n",
    "            'temporal_factors': 0.15,\n",
    "            'environmental_factors': 0.10,\n",
    "            'vehicle_dynamics': 0.10\n",
    "        }\n",
    "        \n",
    "        # Risk thresholds for categorization\n",
    "        self.risk_thresholds = {\n",
    "            'very_low': (0, 20),\n",
    "            'low': (20, 40),\n",
    "            'moderate': (40, 60),\n",
    "            'high': (60, 80),\n",
    "            'very_high': (80, 100)\n",
    "        }\n",
    "        \n",
    "        # Premium adjustment factors\n",
    "        self.premium_factors = {\n",
    "            'very_low': 0.75,    # 25% discount\n",
    "            'low': 0.85,         # 15% discount\n",
    "            'moderate': 1.0,     # Standard rate\n",
    "            'high': 1.25,        # 25% surcharge\n",
    "            'very_high': 1.60    # 60% surcharge\n",
    "        }\n",
    "    \n",
    "    def load_expert_outputs(self, behavior_csv=None, geographic_csv=None, contextual_csv=None):\n",
    "        \"\"\"\n",
    "        Load CSV outputs from all three expert models\n",
    "        \"\"\"\n",
    "        expert_data = {}\n",
    "        \n",
    "        # Load behavior expert output\n",
    "        if behavior_csv:\n",
    "            try:\n",
    "                behavior_df = pd.read_csv(behavior_csv)\n",
    "                expert_data['behavior'] = behavior_df\n",
    "                print(f\"✅ Behavior model data loaded: {len(behavior_df)} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading behavior data: {e}\")\n",
    "                \n",
    "        # Load geographic expert output\n",
    "        if geographic_csv:\n",
    "            try:\n",
    "                geographic_df = pd.read_csv(geographic_csv)\n",
    "                expert_data['geographic'] = geographic_df\n",
    "                print(f\"✅ Geographic model data loaded: {len(geographic_df)} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading geographic data: {e}\")\n",
    "                \n",
    "        # Load contextual expert output\n",
    "        if contextual_csv:\n",
    "            try:\n",
    "                contextual_df = pd.read_csv(contextual_csv)\n",
    "                expert_data['contextual'] = contextual_df\n",
    "                print(f\"✅ Contextual model data loaded: {len(contextual_df)} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading contextual data: {e}\")\n",
    "        \n",
    "        return expert_data\n",
    "    \n",
    "    def create_synthetic_expert_data(self, n_drivers=1000):\n",
    "        \"\"\"\n",
    "        Create synthetic expert model outputs for demonstration\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate driver IDs\n",
    "        driver_ids = range(1, n_drivers + 1)\n",
    "        \n",
    "        # Behavior Expert Output (from comprehensive_telematics_analysis.ipynb structure)\n",
    "        behavior_data = pd.DataFrame({\n",
    "            'driver_id': driver_ids,\n",
    "            'behavior_score': np.random.normal(75, 15, n_drivers).clip(0, 100),\n",
    "            'base_score': np.random.normal(70, 12, n_drivers).clip(0, 100),\n",
    "            'feature_adjustment': np.random.normal(5, 8, n_drivers),\n",
    "            'model_confidence': np.random.uniform(0.6, 0.95, n_drivers),\n",
    "            'predicted_class': np.random.choice(['SLOW', 'NORMAL', 'AGGRESSIVE'], n_drivers, p=[0.2, 0.6, 0.2]),\n",
    "            'risk_category': np.random.choice(['LOW_RISK', 'MODERATE_RISK', 'HIGH_RISK', 'VERY_HIGH_RISK'], \n",
    "                                           n_drivers, p=[0.3, 0.4, 0.25, 0.05]),\n",
    "            'acceleration_std': np.random.uniform(0.1, 2.5, n_drivers),\n",
    "            'braking_intensity': np.random.uniform(0.5, 3.0, n_drivers),\n",
    "            'speed_variance': np.random.uniform(5, 25, n_drivers),\n",
    "            'phone_usage_rate': np.random.uniform(0, 0.3, n_drivers)\n",
    "        })\n",
    "        \n",
    "        # Geographic Expert Output (from geographic_risk_scoring_model.ipynb structure)\n",
    "        geographic_data = pd.DataFrame({\n",
    "            'driver_id': driver_ids,\n",
    "            'geographic_risk_score': np.random.normal(45, 20, n_drivers).clip(0, 100),\n",
    "            'latitude': np.random.uniform(51.4, 51.6, n_drivers),  # London area\n",
    "            'longitude': np.random.uniform(-0.3, 0.1, n_drivers),\n",
    "            'accident_frequency': np.random.poisson(2, n_drivers),\n",
    "            'road_condition_risk': np.random.uniform(20, 80, n_drivers),\n",
    "            'environmental_risk_score': np.random.uniform(10, 70, n_drivers),\n",
    "            'proximity_risk': np.random.uniform(15, 85, n_drivers),\n",
    "            'population_density_risk': np.random.uniform(25, 75, n_drivers),\n",
    "            'risk_category': np.random.choice(['Very Low', 'Low', 'Medium', 'High', 'Very High'], \n",
    "                                           n_drivers, p=[0.1, 0.25, 0.4, 0.2, 0.05])\n",
    "        })\n",
    "        \n",
    "        # Contextual Expert Output (from contextual_risk_scoring_model.ipynb structure)\n",
    "        contextual_data = pd.DataFrame({\n",
    "            'driver_id': driver_ids,\n",
    "            'contextual_risk_score': np.random.normal(40, 18, n_drivers).clip(0, 100),\n",
    "            'temporal_risk': np.random.uniform(20, 80, n_drivers),\n",
    "            'weather_risk': np.random.uniform(10, 90, n_drivers),\n",
    "            'traffic_risk': np.random.uniform(15, 85, n_drivers),\n",
    "            'day_of_week_risk': np.random.uniform(25, 75, n_drivers),\n",
    "            'hour_risk': np.random.uniform(20, 95, n_drivers),\n",
    "            'seasonal_risk': np.random.uniform(30, 70, n_drivers),\n",
    "            'holiday_factor': np.random.uniform(0.8, 1.5, n_drivers),\n",
    "            'rush_hour_indicator': np.random.choice([0, 1], n_drivers, p=[0.7, 0.3])\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'behavior': behavior_data,\n",
    "            'geographic': geographic_data,\n",
    "            'contextual': contextual_data\n",
    "        }\n",
    "    \n",
    "    def combine_expert_scores(self, expert_data):\n",
    "        \"\"\"\n",
    "        Advanced combination of expert scores with feature-level integration\n",
    "        \"\"\"\n",
    "        # Ensure all datasets have the same drivers\n",
    "        common_drivers = set(expert_data['behavior']['driver_id'])\n",
    "        for expert_name, data in expert_data.items():\n",
    "            if expert_name != 'behavior':\n",
    "                common_drivers = common_drivers.intersection(set(data['driver_id']))\n",
    "        \n",
    "        print(f\"Processing {len(common_drivers)} common drivers across all experts\")\n",
    "        \n",
    "        # Create comprehensive combined dataset\n",
    "        combined_results = []\n",
    "        \n",
    "        for driver_id in sorted(common_drivers):\n",
    "            # Extract data for this driver\n",
    "            behavior_row = expert_data['behavior'][expert_data['behavior']['driver_id'] == driver_id].iloc[0]\n",
    "            geo_row = expert_data['geographic'][expert_data['geographic']['driver_id'] == driver_id].iloc[0]\n",
    "            context_row = expert_data['contextual'][expert_data['contextual']['driver_id'] == driver_id].iloc[0]\n",
    "            \n",
    "            # Extract individual scores\n",
    "            behavior_score = behavior_row['behavior_score']\n",
    "            geo_risk = geo_row['geographic_risk_score']\n",
    "            context_risk = context_row['contextual_risk_score']\n",
    "            \n",
    "            # Convert behavior score to risk scale (invert)\n",
    "            behavior_risk = 100 - behavior_score\n",
    "            \n",
    "            # Calculate weighted ensemble score\n",
    "            ensemble_risk = (\n",
    "                behavior_risk * self.expert_weights['behavior'] +\n",
    "                geo_risk * self.expert_weights['geographic'] +\n",
    "                context_risk * self.expert_weights['contextual']\n",
    "            )\n",
    "            \n",
    "            # Calculate feature-based adjustments\n",
    "            feature_adjustment = self._calculate_feature_adjustments(behavior_row, geo_row, context_row)\n",
    "            \n",
    "            # Apply feature adjustments\n",
    "            final_risk_score = np.clip(ensemble_risk + feature_adjustment, 0, 100)\n",
    "            \n",
    "            # Calculate confidence score\n",
    "            confidence_score = self._calculate_confidence(behavior_row, geo_row, context_row)\n",
    "            \n",
    "            # Categorize risk\n",
    "            risk_category = self._categorize_risk(final_risk_score)\n",
    "            \n",
    "            # Calculate premium adjustment\n",
    "            premium_info = self._calculate_premium_adjustment(final_risk_score)\n",
    "            \n",
    "            # Compile comprehensive result\n",
    "            result = {\n",
    "                'driver_id': int(driver_id),\n",
    "                'final_risk_score': round(final_risk_score, 2),\n",
    "                'ensemble_risk_score': round(ensemble_risk, 2),\n",
    "                'feature_adjustment': round(feature_adjustment, 2),\n",
    "                'confidence_score': round(confidence_score, 3),\n",
    "                'risk_category': risk_category,\n",
    "                'premium_factor': premium_info['factor'],\n",
    "                'premium_tier': premium_info['tier'],\n",
    "                \n",
    "                # Individual expert contributions\n",
    "                'behavior_risk': round(behavior_risk, 2),\n",
    "                'geographic_risk': round(geo_risk, 2),\n",
    "                'contextual_risk': round(context_risk, 2),\n",
    "                'behavior_score_original': round(behavior_score, 2),\n",
    "                \n",
    "                # Key features from each expert\n",
    "                'model_confidence': round(behavior_row['model_confidence'], 3),\n",
    "                'predicted_driving_class': behavior_row['predicted_class'],\n",
    "                'acceleration_std': round(behavior_row['acceleration_std'], 3),\n",
    "                'braking_intensity': round(behavior_row['braking_intensity'], 3),\n",
    "                'location_latitude': round(geo_row['latitude'], 6),\n",
    "                'location_longitude': round(geo_row['longitude'], 6),\n",
    "                'accident_frequency': int(geo_row['accident_frequency']),\n",
    "                'road_condition_risk': round(geo_row['road_condition_risk'], 2),\n",
    "                'temporal_risk': round(context_row['temporal_risk'], 2),\n",
    "                'weather_risk': round(context_row['weather_risk'], 2),\n",
    "                'traffic_risk': round(context_row['traffic_risk'], 2),\n",
    "                'rush_hour_indicator': int(context_row['rush_hour_indicator']),\n",
    "                \n",
    "                # Expert weights used\n",
    "                'weight_behavior': self.expert_weights['behavior'],\n",
    "                'weight_geographic': self.expert_weights['geographic'],\n",
    "                'weight_contextual': self.expert_weights['contextual']\n",
    "            }\n",
    "            \n",
    "            combined_results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(combined_results)\n",
    "    \n",
    "    def _calculate_feature_adjustments(self, behavior_row, geo_row, context_row):\n",
    "        \"\"\"\n",
    "        Calculate feature-based adjustments to the ensemble score\n",
    "        \"\"\"\n",
    "        adjustment = 0\n",
    "        \n",
    "        # Behavior-based adjustments\n",
    "        if behavior_row['acceleration_std'] > 2.0:\n",
    "            adjustment += 5  # High acceleration variance = higher risk\n",
    "        if behavior_row['braking_intensity'] > 2.5:\n",
    "            adjustment += 4  # Harsh braking = higher risk\n",
    "        if behavior_row['phone_usage_rate'] > 0.2:\n",
    "            adjustment += 6  # High phone usage = higher risk\n",
    "            \n",
    "        # Geographic-based adjustments\n",
    "        if geo_row['accident_frequency'] > 3:\n",
    "            adjustment += 3  # High accident area = higher risk\n",
    "        if geo_row['road_condition_risk'] > 70:\n",
    "            adjustment += 2  # Poor road conditions = higher risk\n",
    "            \n",
    "        # Contextual-based adjustments\n",
    "        if context_row['weather_risk'] > 75:\n",
    "            adjustment += 3  # Bad weather conditions = higher risk\n",
    "        if context_row['rush_hour_indicator'] == 1:\n",
    "            adjustment += 2  # Rush hour driving = higher risk\n",
    "        if context_row['holiday_factor'] > 1.3:\n",
    "            adjustment += 1  # Holiday periods = slightly higher risk\n",
    "            \n",
    "        return adjustment\n",
    "    \n",
    "    def _calculate_confidence(self, behavior_row, geo_row, context_row):\n",
    "        \"\"\"\n",
    "        Calculate confidence in the ensemble prediction\n",
    "        \"\"\"\n",
    "        # Base confidence from behavior model\n",
    "        base_confidence = behavior_row['model_confidence']\n",
    "        \n",
    "        # Adjust based on data quality and consistency\n",
    "        confidence_factors = []\n",
    "        \n",
    "        # Data completeness factor\n",
    "        confidence_factors.append(0.9)  # Assume good data completeness\n",
    "        \n",
    "        # Geographic data quality\n",
    "        if geo_row['accident_frequency'] >= 0:  # Valid accident data\n",
    "            confidence_factors.append(0.95)\n",
    "        else:\n",
    "            confidence_factors.append(0.8)\n",
    "            \n",
    "        # Contextual data consistency\n",
    "        if 0 <= context_row['weather_risk'] <= 100:\n",
    "            confidence_factors.append(0.9)\n",
    "        else:\n",
    "            confidence_factors.append(0.7)\n",
    "            \n",
    "        # Calculate final confidence\n",
    "        final_confidence = base_confidence * np.mean(confidence_factors)\n",
    "        return np.clip(final_confidence, 0, 1)\n",
    "    \n",
    "    def _categorize_risk(self, risk_score):\n",
    "        \"\"\"Categorize risk level based on score\"\"\"\n",
    "        for category, (min_val, max_val) in self.risk_thresholds.items():\n",
    "            if min_val <= risk_score < max_val:\n",
    "                return category.replace('_', ' ').title()\n",
    "        return \"Very High\"  # Fallback for edge cases\n",
    "    \n",
    "    def _calculate_premium_adjustment(self, risk_score):\n",
    "        \"\"\"Calculate premium adjustment based on risk score\"\"\"\n",
    "        category = self._categorize_risk(risk_score).lower().replace(' ', '_')\n",
    "        factor = self.premium_factors.get(category, 1.0)\n",
    "        \n",
    "        tier_mapping = {\n",
    "            'very_low': 'Preferred Plus',\n",
    "            'low': 'Preferred',\n",
    "            'moderate': 'Standard',\n",
    "            'high': 'Substandard',\n",
    "            'very_high': 'High Risk'\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'factor': factor,\n",
    "            'tier': tier_mapping.get(category, 'Standard')\n",
    "        }\n",
    "\n",
    "# Initialize the comprehensive gating mechanism\n",
    "print(\"Comprehensive Gating Mechanism initialized!\")\n",
    "print(\"Features:\")\n",
    "print(\"- CSV-based expert model integration\")\n",
    "print(\"- Driver ID indexing with natural numbers\")\n",
    "print(\"- Advanced feature-level combination\")\n",
    "print(\"- Confidence scoring\")\n",
    "print(\"- Premium calculation\")\n",
    "print(\"- Comprehensive feature set export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test the comprehensive gating mechanism\n",
    "gating_mechanism = ComprehensiveGatingMechanism()\n",
    "\n",
    "# Generate synthetic expert data for demonstration\n",
    "print(\"Generating synthetic expert model outputs...\")\n",
    "expert_data = gating_mechanism.create_synthetic_expert_data(n_drivers=500)\n",
    "\n",
    "print(\"\\nExpert Data Summary:\")\n",
    "for expert_name, data in expert_data.items():\n",
    "    print(f\"  {expert_name.title()} Expert: {len(data)} drivers, {data.shape[1]} features\")\n",
    "    \n",
    "# Display sample data from each expert\n",
    "print(\"\\nSample Expert Outputs:\")\n",
    "print(\"\\n1. Behavior Expert Sample:\")\n",
    "print(expert_data['behavior'][['driver_id', 'behavior_score', 'predicted_class', 'risk_category']].head())\n",
    "\n",
    "print(\"\\n2. Geographic Expert Sample:\")\n",
    "print(expert_data['geographic'][['driver_id', 'geographic_risk_score', 'latitude', 'longitude', 'risk_category']].head())\n",
    "\n",
    "print(\"\\n3. Contextual Expert Sample:\")\n",
    "print(expert_data['contextual'][['driver_id', 'contextual_risk_score', 'temporal_risk', 'weather_risk']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the comprehensive gating mechanism\n",
    "print(\"Executing Comprehensive Gating Mechanism...\")\n",
    "final_scores_df = gating_mechanism.combine_expert_scores(expert_data)\n",
    "\n",
    "print(f\"\\nGating mechanism completed!\")\n",
    "print(f\"Generated comprehensive scores for {len(final_scores_df)} drivers\")\n",
    "\n",
    "# Display final results summary\n",
    "print(f\"\\nFinal Scoring Summary:\")\n",
    "print(f\"  Average Risk Score: {final_scores_df['final_risk_score'].mean():.2f}\")\n",
    "print(f\"  Risk Score Range: {final_scores_df['final_risk_score'].min():.2f} - {final_scores_df['final_risk_score'].max():.2f}\")\n",
    "print(f\"  Average Confidence: {final_scores_df['confidence_score'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nRisk Category Distribution:\")\n",
    "risk_dist = final_scores_df['risk_category'].value_counts()\n",
    "for category, count in risk_dist.items():\n",
    "    percentage = (count / len(final_scores_df)) * 100\n",
    "    print(f\"  {category}: {count} drivers ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPremium Tier Distribution:\")\n",
    "tier_dist = final_scores_df['premium_tier'].value_counts()\n",
    "for tier, count in tier_dist.items():\n",
    "    percentage = (count / len(final_scores_df)) * 100\n",
    "    print(f\"  {tier}: {count} drivers ({percentage:.1f}%)\")\n",
    "\n",
    "# Display sample of final results\n",
    "print(f\"\\nSample Final Results:\")\n",
    "sample_columns = ['driver_id', 'final_risk_score', 'risk_category', 'premium_tier', \n",
    "                 'behavior_risk', 'geographic_risk', 'contextual_risk', 'confidence_score']\n",
    "print(final_scores_df[sample_columns].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance and Analysis\n",
    "print(\"Analyzing Feature Importance and Correlations...\")\n",
    "\n",
    "def analyze_feature_importance(df):\n",
    "    \"\"\"\n",
    "    Analyze feature importance for the final risk score\n",
    "    \"\"\"\n",
    "    # Select numeric features for correlation analysis\n",
    "    feature_columns = [col for col in df.columns if col not in ['driver_id', 'risk_category', 'premium_tier', 'predicted_driving_class']]\n",
    "    numeric_df = df[feature_columns]\n",
    "    \n",
    "    # Calculate correlations with final risk score\n",
    "    correlations = numeric_df.corr()['final_risk_score'].abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Remove self-correlation\n",
    "    correlations = correlations.drop('final_risk_score')\n",
    "    \n",
    "    print(\"Top 15 Most Important Features:\")\n",
    "    for i, (feature, correlation) in enumerate(correlations.head(15).items(), 1):\n",
    "        print(f\"  {i:2d}. {feature:<25} | Correlation: {correlation:.3f}\")\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = analyze_feature_importance(final_scores_df)\n",
    "\n",
    "# Create comprehensive feature set for application use\n",
    "def create_application_feature_set(df, top_n_features=20):\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set for production application\n",
    "    \"\"\"\n",
    "    # Core identification and scoring features\n",
    "    core_features = [\n",
    "        'driver_id', 'final_risk_score', 'ensemble_risk_score', \n",
    "        'confidence_score', 'risk_category', 'premium_factor', 'premium_tier'\n",
    "    ]\n",
    "    \n",
    "    # Expert contribution features\n",
    "    expert_features = [\n",
    "        'behavior_risk', 'geographic_risk', 'contextual_risk',\n",
    "        'behavior_score_original', 'model_confidence'\n",
    "    ]\n",
    "    \n",
    "    # Top behavioral features\n",
    "    behavioral_features = [\n",
    "        'predicted_driving_class', 'acceleration_std', 'braking_intensity'\n",
    "    ]\n",
    "    \n",
    "    # Top geographic features\n",
    "    geographic_features = [\n",
    "        'location_latitude', 'location_longitude', 'accident_frequency', 'road_condition_risk'\n",
    "    ]\n",
    "    \n",
    "    # Top contextual features\n",
    "    contextual_features = [\n",
    "        'temporal_risk', 'weather_risk', 'traffic_risk', 'rush_hour_indicator'\n",
    "    ]\n",
    "    \n",
    "    # Feature adjustments and weights\n",
    "    meta_features = [\n",
    "        'feature_adjustment', 'weight_behavior', 'weight_geographic', 'weight_contextual'\n",
    "    ]\n",
    "    \n",
    "    # Combine all important features\n",
    "    all_important_features = (core_features + expert_features + behavioral_features + \n",
    "                            geographic_features + contextual_features + meta_features)\n",
    "    \n",
    "    # Create final feature set\n",
    "    final_feature_set = df[all_important_features].copy()\n",
    "    \n",
    "    # Add derived features for application use\n",
    "    final_feature_set['risk_score_normalized'] = final_feature_set['final_risk_score'] / 100\n",
    "    final_feature_set['is_high_risk'] = (final_feature_set['final_risk_score'] >= 60).astype(int)\n",
    "    final_feature_set['is_preferred_customer'] = (final_feature_set['final_risk_score'] <= 30).astype(int)\n",
    "    final_feature_set['expert_agreement'] = (\n",
    "        final_feature_set[['behavior_risk', 'geographic_risk', 'contextual_risk']].std(axis=1)\n",
    "    )\n",
    "    \n",
    "    return final_feature_set\n",
    "\n",
    "# Create application-ready feature set\n",
    "application_features = create_application_feature_set(final_scores_df)\n",
    "\n",
    "print(f\"\\nApplication Feature Set Created:\")\n",
    "print(f\"  Total Features: {application_features.shape[1]}\")\n",
    "print(f\"  Total Drivers: {application_features.shape[0]}\")\n",
    "print(f\"  Memory Usage: {application_features.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nApplication Feature Categories:\")\n",
    "feature_categories = {\n",
    "    'Core Scoring': ['driver_id', 'final_risk_score', 'ensemble_risk_score', 'confidence_score'],\n",
    "    'Risk Classification': ['risk_category', 'premium_factor', 'premium_tier'],\n",
    "    'Expert Contributions': ['behavior_risk', 'geographic_risk', 'contextual_risk'],\n",
    "    'Behavioral Metrics': ['predicted_driving_class', 'acceleration_std', 'braking_intensity'],\n",
    "    'Geographic Factors': ['location_latitude', 'location_longitude', 'accident_frequency'],\n",
    "    'Contextual Factors': ['temporal_risk', 'weather_risk', 'traffic_risk'],\n",
    "    'Derived Features': ['risk_score_normalized', 'is_high_risk', 'is_preferred_customer', 'expert_agreement']\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    available_features = [f for f in features if f in application_features.columns]\n",
    "    print(f\"  {category}: {len(available_features)} features\")\n",
    "\n",
    "# Display sample of application feature set\n",
    "print(f\"\\nApplication Feature Set Sample:\")\n",
    "sample_app_features = ['driver_id', 'final_risk_score', 'risk_category', 'premium_tier', \n",
    "                      'confidence_score', 'behavior_risk', 'geographic_risk', 'contextual_risk',\n",
    "                      'is_high_risk', 'expert_agreement']\n",
    "print(application_features[sample_app_features].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7686806",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.12' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adcd716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the missing visualization function\n",
    "def create_comprehensive_visualizations(final_scores_df, feature_importance):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for the gating mechanism results\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Set style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Comprehensive Gating Mechanism Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Risk Score Distribution\n",
    "    axes[0, 0].hist(final_scores_df['final_risk_score'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Risk Score Distribution')\n",
    "    axes[0, 0].set_xlabel('Final Risk Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Risk Category Distribution\n",
    "    risk_counts = final_scores_df['risk_category'].value_counts()\n",
    "    axes[0, 1].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 1].set_title('Risk Category Distribution')\n",
    "    \n",
    "    # 3. Confidence Score vs Risk Score\n",
    "    axes[0, 2].scatter(final_scores_df['confidence_score'], final_scores_df['final_risk_score'], \n",
    "                      alpha=0.6, s=20)\n",
    "    axes[0, 2].set_title('Confidence vs Risk Score')\n",
    "    axes[0, 2].set_xlabel('Confidence Score')\n",
    "    axes[0, 2].set_ylabel('Final Risk Score')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Feature Importance\n",
    "    top_features = feature_importance.head(10)\n",
    "    axes[1, 0].barh(range(len(top_features)), top_features.values)\n",
    "    axes[1, 0].set_yticks(range(len(top_features)))\n",
    "    axes[1, 0].set_yticklabels(top_features.index, fontsize=8)\n",
    "    axes[1, 0].set_title('Top 10 Feature Importance')\n",
    "    axes[1, 0].set_xlabel('Correlation with Risk Score')\n",
    "    \n",
    "    # 5. Premium Factor Distribution\n",
    "    axes[1, 1].hist(final_scores_df['premium_factor'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[1, 1].set_title('Premium Factor Distribution')\n",
    "    axes[1, 1].set_xlabel('Premium Factor')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Expert Contribution Comparison\n",
    "    expert_cols = ['behavior_risk', 'geographic_risk', 'contextual_risk']\n",
    "    expert_data = final_scores_df[expert_cols].mean()\n",
    "    axes[1, 2].bar(expert_data.index, expert_data.values, color=['coral', 'lightblue', 'lightgreen'])\n",
    "    axes[1, 2].set_title('Average Expert Contributions')\n",
    "    axes[1, 2].set_ylabel('Average Risk Score')\n",
    "    axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Comprehensive visualizations created successfully!\")\n",
    "\n",
    "# Export Results and Model Deployment Preparation\n",
    "print(\"Preparing data export and model deployment...\")\n",
    "\n",
    "def export_comprehensive_results(final_scores_df, application_features, gating_mechanism):\n",
    "    \"\"\"\n",
    "    Export all results to CSV files for production use\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Export complete final scores dataset\n",
    "    complete_filename = f\"comprehensive_gating_results_{timestamp}.csv\"\n",
    "    final_scores_df.to_csv(complete_filename, index=False)\n",
    "    print(f\"Complete gating results exported: {complete_filename}\")\n",
    "    \n",
    "    # 2. Export application-ready feature set\n",
    "    app_filename = f\"application_feature_set_{timestamp}.csv\"\n",
    "    application_features.to_csv(app_filename, index=False)\n",
    "    print(f\"Application feature set exported: {app_filename}\")\n",
    "    \n",
    "    # 3. Export summary statistics\n",
    "    summary_filename = f\"gating_summary_statistics_{timestamp}.csv\"\n",
    "    summary_stats = pd.DataFrame({\n",
    "        'Metric': ['Total Drivers', 'Average Risk Score', 'Average Confidence', \n",
    "                  'High Risk Drivers (%)', 'Preferred Customers (%)', \n",
    "                  'Average Premium Factor'],\n",
    "        'Value': [\n",
    "            len(final_scores_df),\n",
    "            final_scores_df['final_risk_score'].mean(),\n",
    "            final_scores_df['confidence_score'].mean(),\n",
    "            (final_scores_df['final_risk_score'] >= 60).mean() * 100,\n",
    "            (final_scores_df['final_risk_score'] <= 30).mean() * 100,\n",
    "            final_scores_df['premium_factor'].mean()\n",
    "        ],\n",
    "    })\n",
    "    summary_stats.to_csv(summary_filename, index=False)\n",
    "    print(f\"Summary statistics exported: {summary_filename}\")\n",
    "    \n",
    "    # 4. Export model configuration\n",
    "    config_filename = f\"gating_model_config_{timestamp}.json\"\n",
    "    model_config = {\n",
    "        'expert_weights': gating_mechanism.expert_weights,\n",
    "        'feature_importance_weights': gating_mechanism.feature_importance_weights,\n",
    "        'risk_thresholds': gating_mechanism.risk_thresholds,\n",
    "        'premium_factors': gating_mechanism.premium_factors,\n",
    "        'model_version': '1.0',\n",
    "        'creation_date': datetime.now().isoformat(),\n",
    "        'total_drivers_processed': len(final_scores_df),\n",
    "        'feature_count': application_features.shape[1]\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(config_filename, 'w') as f:\n",
    "        json.dump(model_config, f, indent=2)\n",
    "    print(f\"Model configuration exported: {config_filename}\")\n",
    "    \n",
    "    # 5. Create deployment guide\n",
    "    deployment_guide = f\"\"\"\n",
    "COMPREHENSIVE GATING MECHANISM - DEPLOYMENT GUIDE\n",
    "=================================================\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "EXPORTED FILES:\n",
    "1. {complete_filename} - Complete gating results with all features\n",
    "2. {app_filename} - Application-ready feature set\n",
    "3. {summary_filename} - Summary statistics\n",
    "4. {config_filename} - Model configuration\n",
    "\n",
    "KEY FEATURES FOR APPLICATION:\n",
    "- driver_id: Natural number index (1 to N)\n",
    "- final_risk_score: Primary risk score (0-100)\n",
    "- risk_category: Risk classification (Very Low, Low, Moderate, High, Very High)\n",
    "- premium_factor: Insurance premium adjustment factor\n",
    "- confidence_score: Model confidence (0-1)\n",
    "- expert contributions: Individual expert risk scores\n",
    "\n",
    "MODEL PERFORMANCE:\n",
    "- Total Drivers Processed: {len(final_scores_df):,}\n",
    "- Average Risk Score: {final_scores_df['final_risk_score'].mean():.2f}\n",
    "- Average Confidence: {final_scores_df['confidence_score'].mean():.3f}\n",
    "- Feature Count: {application_features.shape[1]}\n",
    "\n",
    "INTEGRATION NOTES:\n",
    "1. Load application_feature_set CSV for production scoring\n",
    "2. Use driver_id as primary key for customer lookup\n",
    "3. Apply premium_factor to base insurance rates\n",
    "4. Monitor confidence_score for model reliability\n",
    "5. Retrain when confidence drops below 0.7\n",
    "\n",
    "EXPERT WEIGHTS USED:\n",
    "- Behavior Expert: {gating_mechanism.expert_weights['behavior']:.1%}\n",
    "- Geographic Expert: {gating_mechanism.expert_weights['geographic']:.1%}\n",
    "- Contextual Expert: {gating_mechanism.expert_weights['contextual']:.1%}\n",
    "\"\"\"\n",
    "    \n",
    "    guide_filename = f\"deployment_guide_{timestamp}.txt\"\n",
    "    with open(guide_filename, 'w') as f:\n",
    "        f.write(deployment_guide)\n",
    "    print(f\"Deployment guide created: {guide_filename}\")\n",
    "    \n",
    "    return {\n",
    "        'complete_results': complete_filename,\n",
    "        'application_features': app_filename,\n",
    "        'summary_stats': summary_filename,\n",
    "        'model_config': config_filename,\n",
    "        'deployment_guide': guide_filename\n",
    "    }\n",
    "\n",
    "# Export all results\n",
    "exported_files = export_comprehensive_results(final_scores_df, application_features, gating_mechanism)\n",
    "\n",
    "print(f\"\\nGATING MECHANISM IMPLEMENTATION COMPLETED!\")\n",
    "print(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
    "print(f\"Processed {len(final_scores_df)} drivers with natural number IDs\")\n",
    "print(f\"Combined outputs from 3 expert models\")\n",
    "print(f\"Generated comprehensive feature set with {application_features.shape[1]} features\")\n",
    "print(f\"Exported production-ready CSV files\")\n",
    "print(f\"Created deployment documentation\")\n",
    "\n",
    "print(f\"\\nQUICK STATS:\")\n",
    "print(f\"  Risk Score Range: {final_scores_df['final_risk_score'].min():.1f} - {final_scores_df['final_risk_score'].max():.1f}\")\n",
    "print(f\"  Average Confidence: {final_scores_df['confidence_score'].mean():.3f}\")\n",
    "print(f\"  Risk Categories: {final_scores_df['risk_category'].nunique()} levels\")\n",
    "print(f\"  Premium Tiers: {final_scores_df['premium_tier'].nunique()} tiers\")\n",
    "\n",
    "print(f\"\\nEXPORTED FILES:\")\n",
    "for file_type, filename in exported_files.items():\n",
    "    print(f\"  {file_type.replace('_', ' ').title()}: {filename}\")\n",
    "\n",
    "print(f\"\\nReady for production deployment!\")\n",
    "print(f\"Use the application_feature_set CSV for real-time scoring in your insurance application.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "print(\"Creating comprehensive visualizations...\")\n",
    "create_comprehensive_visualizations(final_scores_df, feature_importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
