{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05c337c",
   "metadata": {},
   "source": [
    "# Gating Model - Expert Ensemble\n",
    "Reference: how-can-we-prevent-road-rage (merging outputs)\n",
    "\n",
    "This notebook implements the gating model that combines outputs from all expert models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f1d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Gating Model - Expert Ensemble - Ready for implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5b2c0",
   "metadata": {},
   "source": [
    "## Expert Model Integration\n",
    "\n",
    "Combine outputs from:\n",
    "1. Behavior Expert\n",
    "2. Geographic Expert\n",
    "3. Contextual Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertEnsemble:\n",
    "    \"\"\"\n",
    "    Ensemble model combining expert outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.expert_weights = {\n",
    "            'behavior': 0.4,\n",
    "            'geographic': 0.3,\n",
    "            'contextual': 0.3\n",
    "        }\n",
    "        \n",
    "        self.risk_thresholds = {\n",
    "            'low': 30,\n",
    "            'moderate': 60,\n",
    "            'high': 80\n",
    "        }\n",
    "    \n",
    "    def combine_expert_scores(self, behavior_score, geo_risk, context_risk):\n",
    "        \"\"\"\n",
    "        Weighted combination of expert scores\n",
    "        \"\"\"\n",
    "        # Convert scores to same scale (risk scores)\n",
    "        behavior_risk = 100 - behavior_score  # Invert behavior score\n",
    "        \n",
    "        # Weighted average\n",
    "        combined_risk = (\n",
    "            behavior_risk * self.expert_weights['behavior'] +\n",
    "            geo_risk * self.expert_weights['geographic'] +\n",
    "            context_risk * self.expert_weights['contextual']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'combined_risk': combined_risk,\n",
    "            'risk_category': self._categorize_risk(combined_risk),\n",
    "            'expert_contributions': {\n",
    "                'behavior': behavior_risk,\n",
    "                'geographic': geo_risk,\n",
    "                'contextual': context_risk\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _categorize_risk(self, risk_score):\n",
    "        \"\"\"Categorize risk level\"\"\"\n",
    "        if risk_score < self.risk_thresholds['low']:\n",
    "            return \"Low Risk\"\n",
    "        elif risk_score < self.risk_thresholds['moderate']:\n",
    "            return \"Moderate Risk\"\n",
    "        elif risk_score < self.risk_thresholds['high']:\n",
    "            return \"High Risk\"\n",
    "        else:\n",
    "            return \"Very High Risk\"\n",
    "\n",
    "# Test the ensemble\n",
    "ensemble = ExpertEnsemble()\n",
    "test_result = ensemble.combine_expert_scores(\n",
    "    behavior_score=85,\n",
    "    geo_risk=45,\n",
    "    context_risk=55\n",
    ")\n",
    "print(f\"Ensemble test result: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f6abb",
   "metadata": {},
   "source": [
    "## Premium Calculation\n",
    "\n",
    "Convert risk scores to insurance premium adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e51db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_premium_adjustment(risk_score, base_premium=1000):\n",
    "    \"\"\"\n",
    "    Calculate insurance premium based on combined risk score\n",
    "    \"\"\"\n",
    "    # Premium adjustment curve\n",
    "    if risk_score < 30:\n",
    "        adjustment_factor = 0.8  # 20% discount\n",
    "        tier = \"Preferred\"\n",
    "    elif risk_score < 50:\n",
    "        adjustment_factor = 0.9  # 10% discount\n",
    "        tier = \"Standard Plus\"\n",
    "    elif risk_score < 70:\n",
    "        adjustment_factor = 1.0  # Standard rate\n",
    "        tier = \"Standard\"\n",
    "    elif risk_score < 85:\n",
    "        adjustment_factor = 1.2  # 20% surcharge\n",
    "        tier = \"Substandard\"\n",
    "    else:\n",
    "        adjustment_factor = 1.5  # 50% surcharge\n",
    "        tier = \"High Risk\"\n",
    "    \n",
    "    adjusted_premium = base_premium * adjustment_factor\n",
    "    savings = base_premium - adjusted_premium\n",
    "    \n",
    "    return {\n",
    "        'base_premium': base_premium,\n",
    "        'adjusted_premium': adjusted_premium,\n",
    "        'adjustment_factor': adjustment_factor,\n",
    "        'savings': savings,\n",
    "        'tier': tier\n",
    "    }\n",
    "\n",
    "# Test premium calculation\n",
    "premium_result = calculate_premium_adjustment(test_result['combined_risk'])\n",
    "print(f\"Premium calculation: {premium_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b4c87",
   "metadata": {},
   "source": [
    "## Model Performance Analysis\n",
    "\n",
    "Analyze expert model contributions and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f1f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_expert_contributions(expert_scores_df):\n",
    "    \"\"\"\n",
    "    Analyze how each expert model contributes to final scores\n",
    "    \"\"\"\n",
    "    # Calculate correlations between expert scores\n",
    "    correlations = expert_scores_df[['behavior', 'geographic', 'contextual']].corr()\n",
    "    \n",
    "    # Feature importance (simplified)\n",
    "    importance = {\n",
    "        'behavior': expert_scores_df['behavior'].std(),\n",
    "        'geographic': expert_scores_df['geographic'].std(),\n",
    "        'contextual': expert_scores_df['contextual'].std()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'correlations': correlations,\n",
    "        'variability': importance\n",
    "    }\n",
    "\n",
    "# Create sample data for analysis\n",
    "sample_data = pd.DataFrame({\n",
    "    'behavior': np.random.normal(75, 15, 100),\n",
    "    'geographic': np.random.normal(50, 20, 100),\n",
    "    'contextual': np.random.normal(45, 18, 100)\n",
    "})\n",
    "\n",
    "analysis_result = analyze_expert_contributions(sample_data)\n",
    "print(\"Expert contribution analysis completed\")\n",
    "print(f\"Correlations:\\n{analysis_result['correlations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95309f84",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize expert model outputs and ensemble results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f105fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expert_distributions(expert_scores_df):\n",
    "    \"\"\"\n",
    "    Plot distributions of expert model scores\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Individual expert distributions\n",
    "    expert_scores_df['behavior'].hist(ax=axes[0,0], bins=20, alpha=0.7)\n",
    "    axes[0,0].set_title('Behavior Scores')\n",
    "    \n",
    "    expert_scores_df['geographic'].hist(ax=axes[0,1], bins=20, alpha=0.7)\n",
    "    axes[0,1].set_title('Geographic Risk Scores')\n",
    "    \n",
    "    expert_scores_df['contextual'].hist(ax=axes[1,0], bins=20, alpha=0.7)\n",
    "    axes[1,0].set_title('Contextual Risk Scores')\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    sns.heatmap(expert_scores_df[['behavior', 'geographic', 'contextual']].corr(), \n",
    "                annot=True, ax=axes[1,1], cmap='coolwarm', center=0)\n",
    "    axes[1,1].set_title('Expert Score Correlations')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Expert distribution plots created\")\n",
    "\n",
    "# Create visualization\n",
    "plot_expert_distributions(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda01cce",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "\n",
    "Optimize expert weights based on performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_expert_weights(expert_scores, actual_claims):\n",
    "    \"\"\"\n",
    "    Optimize expert weights based on claim prediction accuracy\n",
    "    \"\"\"\n",
    "    # This would be implemented with actual claims data\n",
    "    # Using optimization algorithms to find best weights\n",
    "    \n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    def objective(weights):\n",
    "        # Calculate combined scores with given weights\n",
    "        combined_scores = (\n",
    "            expert_scores['behavior'] * weights[0] +\n",
    "            expert_scores['geographic'] * weights[1] +\n",
    "            expert_scores['contextual'] * weights[2]\n",
    "        )\n",
    "        \n",
    "        # Return negative correlation (to maximize)\n",
    "        correlation = np.corrcoef(combined_scores, actual_claims)[0, 1]\n",
    "        return -correlation if not np.isnan(correlation) else 1\n",
    "    \n",
    "    # Constraints: weights sum to 1\n",
    "    constraints = {'type': 'eq', 'fun': lambda x: sum(x) - 1}\n",
    "    bounds = [(0, 1), (0, 1), (0, 1)]\n",
    "    \n",
    "    # Initial weights\n",
    "    initial_weights = [0.4, 0.3, 0.3]\n",
    "    \n",
    "    # Note: This is a placeholder - would use real claims data\n",
    "    print(\"Weight optimization framework ready (requires real claims data)\")\n",
    "    \n",
    "    return initial_weights\n",
    "\n",
    "# Test optimization framework\n",
    "optimized_weights = optimize_expert_weights(sample_data, np.random.random(100))\n",
    "print(f\"Optimized weights: {optimized_weights}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
